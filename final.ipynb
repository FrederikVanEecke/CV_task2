{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b47b15de-64a5-4fa9-a688-23d3efa9a2f4",
    "_uuid": "0cc385a7-98f6-4883-96eb-7b89c7c9aa1c",
    "papermill": {
     "duration": 0.010809,
     "end_time": "2021-03-29T09:04:17.390934",
     "exception": false,
     "start_time": "2021-03-29T09:04:17.380125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"width:100%; height:140px\">\n",
    "    <img src=\"https://www.kuleuven.be/internationaal/thinktank/fotos-en-logos/ku-leuven-logo.png/image_preview\" width = 300px, heigh = auto align=left>\n",
    "</div>\n",
    "\n",
    "\n",
    "KUL H02A5a Computer Vision: Group Assignment 1\n",
    "---------------------------------------------------------------\n",
    "Student numbers: <span style=\"color:red\">r0611202, r0633222, r0621101, r4, r5</span>. (fill in your student numbers!)\n",
    "\n",
    "In this group assignment your team will delve into some deep learning applications for computer vision. The assignment will be delivered in the same groups from *Group assignment 0* and you start from this template notebook. The notebook you submit for grading is the last notebook you submit in the [Kaggle competition](https://www.kaggle.com/t/b7a2a8743bd842ca9ac93ae91cbc8d9f) prior to the deadline on **Tuesday 18 May 23:59**. Closely follow [these instructions](https://github.com/gourie/kaggle_inclass) for joining the competition, sharing your notebook with the TAs and making a valid notebook submission to the competition. A notebook submission not only produces a *submission.csv* file that is used to calculate your competition score, it also runs the entire notebook and saves its output as if it were a report. This way it becomes an all-in-one-place document for the TAs to review. As such, please make sure that your final submission notebook is self-contained and fully documented (e.g. provide strong arguments for the design choices that you make). Most likely, this notebook format is not appropriate to run all your experiments at submission time (e.g. the training of CNNs is a memory hungry and time consuming process; due to limited Kaggle resources). It can be a good idea to distribute your code otherwise and only summarize your findings, together with your final predictions, in the submission notebook. For example, you can substitute experiments with some text and figures that you have produced \"offline\" (e.g. learning curves and results on your internal validation set or even the test set for different architectures, pre-processing pipelines, etc). We advise you to first go through the PDF of this assignment entirely before you really start. Then, it can be a good idea to go through this notebook and use it as your first notebook submission to the competition. You can make use of the *Group assignment 1* forum/discussion board on Toledo if you have any questions. Good luck and have fun!\n",
    "\n",
    "---------------------------------------------------------------\n",
    "NOTES:\n",
    "* This notebook is just a template. Please keep the five main sections, but feel free to adjust further in any way you please!\n",
    "* Clearly indicate the improvements that you make! You can for instance use subsections like: *3.1. Improvement: applying loss function f instead of g*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "35358cfb-b13d-4277-8dd5-4e663c8cd775",
    "_uuid": "3b40b846-d7da-46d8-b354-c6d5c5ded56e",
    "papermill": {
     "duration": 0.009414,
     "end_time": "2021-03-29T09:04:17.410239",
     "exception": false,
     "start_time": "2021-03-29T09:04:17.400825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "This assignment consists of *three main parts* for which we expect you to provide code and extensive documentation in the notebook:\n",
    "* Image classification (Sect. 2)\n",
    "* Semantic segmentation (Sect. 3)\n",
    "* Adversarial attacks (Sect. 4)\n",
    "\n",
    "In the first part, you will train an end-to-end neural network for image classification. In the second part, you will do the same for semantic segmentation. For these two tasks we expect you to put a significant effort into optimizing performance and as such competing with fellow students via the Kaggle competition. In the third part, you will try to find and exploit the weaknesses of your classification and/or segmentation network. For the latter there is no competition format, but we do expect you to put significant effort in achieving good performance on the self-posed goal for that part. Finally, we ask you to reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision (Sect. 5). It is important to note that only a small part of the grade will reflect the actual performance of your networks. However, we do expect all things to work! In general, we will evaluate the correctness of your approach and your understanding of what you have done that you demonstrate in the descriptions and discussions in the final notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009248,
     "end_time": "2021-03-29T09:04:17.429068",
     "exception": false,
     "start_time": "2021-03-29T09:04:17.419820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.1 Deep learning resources\n",
    "If you did not yet explore this in *Group assignment 0 (Sect. 2)*, we recommend using the TensorFlow and/or Keras library for building deep learning models. You can find a nice crash course [here](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-26T19:28:31.852850Z",
     "start_time": "2021-04-26T19:28:30.417480Z"
    },
    "_cell_guid": "7ddf657a-b938-4a49-87dc-b0db9af9156d",
    "_uuid": "c65ea4f1-cc90-408f-b8e0-7c7399ec7e21",
    "execution": {
     "iopub.execute_input": "2021-03-29T09:04:17.454356Z",
     "iopub.status.busy": "2021-03-29T09:04:17.453623Z",
     "iopub.status.idle": "2021-03-29T09:04:23.936131Z",
     "shell.execute_reply": "2021-03-29T09:04:23.936637Z"
    },
    "papermill": {
     "duration": 6.498173,
     "end_time": "2021-03-29T09:04:23.936979",
     "exception": false,
     "start_time": "2021-03-29T09:04:17.438806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "import tensorflow as tf\n",
    "%pip install --upgrade tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "import os \n",
    "import multiprocessing\n",
    "import wandb\n",
    "# !pip install wandb -qqq\n",
    "from wandb.keras import WandbCallback\n",
    "import kerastuner as kt\n",
    "import cv2\n",
    "from ipywidgets import fixed, interact \n",
    "import ipywidgets\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
    "    RandomBrightness, RandomContrast, RandomGamma,\n",
    "    ToFloat, ShiftScaleRotate, RandomBrightnessContrast, RandomCrop)\n",
    "\n",
    "from data_utils import Dataset_Segmentation, Dataset_Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009505,
     "end_time": "2021-03-29T09:04:23.956792",
     "exception": false,
     "start_time": "2021-03-29T09:04:23.947287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.2 PASCAL VOC 2009\n",
    "For this project you will be using the [PASCAL VOC 2009](http://host.robots.ox.ac.uk/pascal/VOC/voc2009/index.html) dataset. This dataset consists of colour images of various scenes with different object classes (e.g. animal: *bird, cat, ...*; vehicle: *aeroplane, bicycle, ...*), totalling 20 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-26T19:28:39.826575Z",
     "start_time": "2021-04-26T19:28:33.696577Z"
    },
    "_cell_guid": "1ce67f49-6bf6-4e5c-b5e4-576e893616a9",
    "_uuid": "3b1c5fbb-757f-4349-b224-e281c540e1ad",
    "execution": {
     "iopub.execute_input": "2021-03-29T09:04:23.987073Z",
     "iopub.status.busy": "2021-03-29T09:04:23.984740Z",
     "iopub.status.idle": "2021-03-29T09:04:54.774578Z",
     "shell.execute_reply": "2021-03-29T09:04:54.775506Z"
    },
    "papermill": {
     "duration": 30.809201,
     "end_time": "2021-03-29T09:04:54.775815",
     "exception": false,
     "start_time": "2021-03-29T09:04:23.966614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the training data\n",
    "train_df = pd.read_csv('data/train/train_set.csv', index_col=\"Id\")\n",
    "labels = train_df.columns\n",
    "train_df[\"img\"] = [np.load('data/train/img/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "train_df[\"seg\"] = [np.load('data/train/seg/train_{}.npy'.format(idx)) for idx, _ in train_df.iterrows()]\n",
    "print(\"The training set contains {} examples.\".format(len(train_df)))\n",
    "\n",
    "# Show some examples\n",
    "fig, axs = plt.subplots(2, 20, figsize=(10 * 20, 10 * 2))\n",
    "for i, label in enumerate(labels):\n",
    "    df = train_df.loc[train_df[label] == 1]\n",
    "    axs[0, i].imshow(df.iloc[0][\"img\"], vmin=0, vmax=255)\n",
    "    axs[0, i].set_title(\"\\n\".join(label for label in labels if df.iloc[0][label] == 1), fontsize=40)\n",
    "    axs[0, i].axis(\"off\")\n",
    "    axs[1, i].imshow(df.iloc[0][\"seg\"], vmin=0, vmax=20)  # with the absolute color scale it will be clear that the arrays in the \"seg\" column are label maps (labels in [0, 20])\n",
    "    axs[1, i].axis(\"off\")\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# The training dataframe contains for each image 20 columns with the ground truth classification labels and 20 column with the ground truth segmentation maps for each class\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-26T19:29:30.108848Z",
     "start_time": "2021-04-26T19:29:28.160299Z"
    },
    "execution": {
     "iopub.execute_input": "2021-03-29T09:04:55.411245Z",
     "iopub.status.busy": "2021-03-29T09:04:55.405662Z",
     "iopub.status.idle": "2021-03-29T09:05:15.866350Z",
     "shell.execute_reply": "2021-03-29T09:05:15.865480Z"
    },
    "papermill": {
     "duration": 20.776375,
     "end_time": "2021-03-29T09:05:15.866678",
     "exception": false,
     "start_time": "2021-03-29T09:04:55.090303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the test data\n",
    "test_df = pd.read_csv('data/test/test_set.csv', index_col=\"Id\")\n",
    "test_df[\"img\"] = [np.load('data/test/img/test_{}.npy'.format(idx)) for idx, _ in test_df.iterrows()]\n",
    "test_df[\"seg\"] = [-1 * np.ones(img.shape[:2], dtype=np.int8) for img in test_df[\"img\"]]\n",
    "print(\"The test set contains {} examples.\".format(len(test_df)))\n",
    "\n",
    "# The test dataframe is similar to the training dataframe, but here the values are -1 --> your task is to fill in these as good as possible in Sect. 2 and Sect. 3; in Sect. 6 this dataframe is automatically transformed in the submission CSV!\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.295407,
     "end_time": "2021-03-29T09:05:16.448937",
     "exception": false,
     "start_time": "2021-03-29T09:05:16.153530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1.3 Your Kaggle submission\n",
    "Your filled test dataframe (during Sect. 2 and Sect. 3) must be converted to a submission.csv with two rows per example (one for classification and one for segmentation) and with only a single prediction column (the multi-class/label predictions running length encoded). You don't need to edit this section. Just make sure to call this function at the right position in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-26T19:30:30.019622Z",
     "start_time": "2021-04-26T19:30:30.013827Z"
    },
    "execution": {
     "iopub.execute_input": "2021-03-29T09:05:17.026801Z",
     "iopub.status.busy": "2021-03-29T09:05:17.024925Z",
     "iopub.status.idle": "2021-03-29T09:05:17.042851Z",
     "shell.execute_reply": "2021-03-29T09:05:17.040634Z"
    },
    "papermill": {
     "duration": 0.293321,
     "end_time": "2021-03-29T09:05:17.043036",
     "exception": false,
     "start_time": "2021-03-29T09:05:16.749715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _rle_encode(img):\n",
    "    \"\"\"\n",
    "    Kaggle requires RLE encoded predictions for computation of the Dice score (https://www.kaggle.com/lifa08/run-length-encode-and-decode)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img: np.ndarray - binary img array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    rle: String - running length encoded version of img\n",
    "    \"\"\"\n",
    "    pixels = img.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    rle = ' '.join(str(x) for x in runs)\n",
    "    return rle\n",
    "\n",
    "def generate_submission(df):\n",
    "    \"\"\"\n",
    "    Make sure to call this function once after you completed Sect. 2 and Sect. 3! It transforms and writes your test dataframe into a submission.csv file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame - filled dataframe that needs to be converted\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    submission_df: pd.DataFrame - df in submission format.\n",
    "    \"\"\"\n",
    "    df_dict = {\"Id\": [], \"Predicted\": []}\n",
    "    for idx, _ in df.iterrows():\n",
    "        df_dict[\"Id\"].append(f\"{idx}_classification\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array(df.loc[idx, labels])))\n",
    "        df_dict[\"Id\"].append(f\"{idx}_segmentation\")\n",
    "        df_dict[\"Predicted\"].append(_rle_encode(np.array([df.loc[idx, \"seg\"] == j + 1 for j in range(len(labels))])))\n",
    "    \n",
    "    submission_df = pd.DataFrame(data=df_dict, dtype=str).set_index(\"Id\")\n",
    "    submission_df.to_csv(\"submission.csv\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Taking a look at the data first:</b> <br>\n",
    "Training usefull classification and segmentation models always starts by inspecting the data available. Within this section we perform such an evaluation on our training set. \n",
    "Thinks to consider are: \n",
    "<ol>\n",
    "  <li>The distribution of classes within the data. In total 20 classes are considered, plus an additional background class for segmentation. These classes should be equally represented within the data, ideally each class would correpsond to 5% of all images in this particular case. This is important since models will be biased towards over-represented classes within the data.</li>\n",
    "  <li>What labels are provided and are they accurate?</li>\n",
    "  <li>Is there enough data, is data augmenetation necessary? </li>\n",
    "</ol>\n",
    "    \n",
    "These questions will be answered within this section\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Class distribution</b> <br>\n",
    "The figure below shows the distribution of classes within the training data. It becomes clear how around 25% of all images are labelled as containing a person. In fact this is not entirely true since we're dealing with a multi-label classification problem, meaning that an image is assigned a set of labels. However, the conclussion remains, the person class is over-represented within the data. We should keep this in mind when defining models. Likewise we also note that the classes sheep and cow are somewhat under represented within the data.\n",
    "    \n",
    "<b>Additional remarks</b> <br>\n",
    "<ol>\n",
    "  <li>Images contained within the training data have various different size. For training all of these are resized to a fixed size, default set to 224 width and height. We should however remain carefull when resizing data, in particular for our segmentation problems. Literature states that maintaining the aspect ratio is important for good segmentation. Therefore padding images to a fixed size might be beneficial for segmentation.</li>\n",
    "  <li>Within our training data we've included a train versus validation split. By default we use 10% for validating.     </li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    'train_fraction': 0.9,\n",
    "    'input_shape': (224, 224),\n",
    "    'augmentation': True, \n",
    "    'uniform_sample_probabilities': False\n",
    "}\n",
    "ds = Dataset_Classification(dataset_config)\n",
    "ds.show_class_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Comparing original and processed data</b> <br>\n",
    "    Within the cell below you can visualize the data and look at the result after preprocessing. Uptill now preprocessing only consists out of resizing. However pretrained models always come with a preprocess function, rescaling the input from the 8 bit integer range into the 0 to 1 or -1 to 1 range. Such a function can be fed into our dataset class and will be called when preprocessing is done. In comment we give an example preprocess function,  uncommenting it will influence the preprocessed data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = ipywidgets.IntText(min=0, max=ds.get_train_set_size(), value=0, label='image_id')\n",
    "#ds.feed_preprocess_function(lambda x: x/255.0) \n",
    "option = ipywidgets.Dropdown(\n",
    "    options=['train', 'test'],\n",
    "    value='train',\n",
    "    description='train or test:',\n",
    "    disabled=False,\n",
    ")\n",
    "interact(ds.view_preprocessed_image,  image_id=image_id, option=option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Data Augmentation</b> <br>\n",
    "You might have already noticed that the original and preprocessed data can differ sometimes. This because data augmentation is enabled by default. Augmentation our data is necessary due to the limit amount of data and high probablity of overfitting on it. Augmentations are done using the albumentations module, each agmentation is defined by an action and a probablity. \n",
    "    \n",
    "<b>Augmentations done</b> <br>\n",
    "<ol>\n",
    "  <li>Horizontal flip, probability 50%</li>\n",
    "  <li>Random contrast: probability 25%</li>\n",
    "  <li>Random brightness: probability 50%</li>\n",
    "  <li>Random shift: probability 80% with a limit of 10% of the image width</li>\n",
    "  <li>Random rotation: probability 80% with a limit of 15 degrees</li>\n",
    "</ol>\n",
    "    \n",
    "    \n",
    "The cell below shows each image from the training data and possible augmentations derived from it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = ipywidgets.IntText(min=0, max=ds.get_train_set_size(), value=0, label='image_id')\n",
    "interact(ds.view_possible_augmentations,  image_id=image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the labels for segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>semantic segmentation labels</b> <br>\n",
    "The assignemnt is centerred around semantic segmentation, the problem of segmenting all classes within an image. For this we need a label with the same size as the image, for which each pixel has the value of the corresponding class. Semantic segmentation is very different from instance segmentation, where all instaces of different classes are segmented seperatly. For this each pixel corresponding to a particular instance is given a different label. We've should make sure that these semantic labels of the data remain valid after applying augmentation.\n",
    "    \n",
    "Within the cells below you can visualize the original, preprocessed and semantic mask for the preprocessed image.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    'train_fraction': 0.9,\n",
    "    'input_shape': (224, 224),\n",
    "    'augmentation': True, \n",
    "    'uniform_sample_probabilities':True\n",
    "}\n",
    "ds = Dataset_Segmentation(dataset_config)\n",
    "#ds.feed_preprocess_function(lambda x: x/255.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = ipywidgets.IntText(min=0, max=ds.get_train_set_size(), value=0, label='image_id')\n",
    "option = ipywidgets.Dropdown(\n",
    "    options=['train', 'test'],\n",
    "    value='train',\n",
    "    description='train or test:',\n",
    "    disabled=False,\n",
    ")\n",
    "interact(ds.view_preprocessed_image,  image_id=image_id, option=option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Augmentation affects the segmentation labels</b> <br>\n",
    "With the cell below we want to illustrate how the semantic labels remain valid after applying augmentatio. The augmentations influencing the semantic label are rotations, translation and rescalings.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = ipywidgets.IntText(min=0, max=ds.get_train_set_size(), value=0, label='image_id')\n",
    "interact(ds.view_possible_augmentations,  image_id=image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.261504,
     "end_time": "2021-03-29T09:05:17.563178",
     "exception": false,
     "start_time": "2021-03-29T09:05:17.301674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image classification\n",
    "The goal here is simple: implement a classification CNN and train it to recognise all 20 classes (and/or background) using the training set and compete on the test set (by filling in the classification columns in the test dataframe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Multi label classification:</b> <br>\n",
    "    The first section will focus on the most succesfull classification models considered. We've focussed on models provided within the keras applications. The models available have proven their capabilities and are well documented. Additionally pretrained weights from imagenet are availabel for all these models.\n",
    "    \n",
    "Our base classification model class includes a high number of methods, therefore we import it instead of defining it over here. This base class takes care of most things, most importantly the head model. This are the final (dense) layers added on top of the base model. The strategy used is to inherit from this base class and define a specific base model. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import ClassifactionModel\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Xception</b> <br>\n",
    "Our first model used will be Xception, we load it from keras and do not include the top layers. Optionally we could load the pretrained weights. this is done when finetuning the model. Within the cell below we define the model and set the configurations. We will a initialize a model obtained through finetuning on the imagenet weights, this model head no dense layers within the head model. We just added one final layer corresponding to the number of classes.  \n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-26T19:32:04.869Z"
    },
    "code_folding": [
     18,
     30,
     43,
     163,
     181,
     215,
     279,
     285,
     390,
     432,
     479,
     533,
     566,
     581,
     592,
     613,
     626,
     641,
     664,
     697,
     705,
     741,
     763,
     766,
     794
    ]
   },
   "outputs": [],
   "source": [
    "class XceptionModel(ClassifactionModel): \n",
    "    def __init__(self, config):\n",
    "        # setup model name for wandb\n",
    "        self.project_name = 'Xception'\n",
    "        self.heatmap_layer_name='block14_sepconv2_act'\n",
    "        # define the base model\n",
    "        self.base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                               include_top=False)\n",
    "        # super takes care of the rest\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # feed preprocessor function \n",
    "        self.dataset.feed_preprocess_function(keras.applications.xception.preprocess_input)\n",
    "        \n",
    "config_xception = {\n",
    "    'name': 'XceptionModel',\n",
    "    'logging_wandb': False,  #nice tool for tracking a run. tensorboad-like\n",
    "    'weights': \"imagenet\", # 'imagenet', #None, \n",
    "    'nbr_classes': 20,\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_base_model': True, # whether to train the head and or base model\n",
    "    'train_head_model': True, \n",
    "    'train_parameters': {\n",
    "        'optimizer': 'ADAM',\n",
    "        'epochs': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.00001, \n",
    "        'steps_per_epoch': 2000\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    },\n",
    "    'head_model': {\n",
    "        'head_model_units': [], \n",
    "        'add_dropout':      [],\n",
    "        'activation': 'relu',\n",
    "        'output_activation': 'softmax'\n",
    "    }\n",
    "}\n",
    "import tensorflow as tf\n",
    "# # GPU \n",
    "# gpus = tf.config.list_physical_devices('GPU')\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         # Currently, memory growth needs to be the same across GPUs\n",
    "#         for gpu in gpus:\n",
    "#             tf.config.experimental.set_memory_growth(gpu, True)\n",
    "#         logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "#         print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "#     except RuntimeError as e:\n",
    "#         # Memory growth must be set before GPUs have been initialized\n",
    "#         print(e)\n",
    "        \n",
    "#CPU\n",
    "# my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')\n",
    "# tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Inference</b> <br>\n",
    "We will not train the model again but just us it for inference, the cpu is used to avoid problems with the gpu. We show predictions made on the test by outputting its most confident prediction, along with a hint on where the model is basing this prediction on. For this we show the activations of a predefined convolutonal layer from within the model. Such a visualization can help in understanding why the model is behaving in a certain way.\n",
    "\n",
    "<b>Interesting image to look at:</b> <br>\n",
    "<ol>\n",
    "  <li>Image_id=1: The image shows multiple classes present within the training data such as potted plant, tables and chairs. Finally it predicts the chair as most confident predection. By looking at the activation is becomes clear that the model is indeed focussing on the part of the image where the chair is.</li>\n",
    "  <li>Image_id=4, 6, 35: Same as previously. Within these images the activation also explains why a particular prediction is made when multiple classes are present.</li>\n",
    "  <li>Image_id=12: It's clear how the model is basing its prediction on the cat its face.</li>\n",
    "  <li>Image_id=25: The predicted boat is accurate, however the focus of the model for making this prediction is questionable. The activations do not focus on the boat but on the water. This is a commonly encoutered, the model has learned to predict boat when it sees water. Meaning that this prediction is in fact based on the context of a boat, mostly found within water. </li>\n",
    "  <li>Image_id=6: This prediction supports the argument made above. The prediction is still accurate, however much less confident.</li>\n",
    "  <li>Image_id=38: Our model is still confusing cats and dogs..</li>\n",
    "</ol>\n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-26T19:26:21.498861Z",
     "start_time": "2021-04-26T19:26:21.479091Z"
    },
    "code_folding": [
     2
    ]
   },
   "outputs": [],
   "source": [
    "Xception = XceptionModel(config_xception)\n",
    "Xception.prepare_for_inference('weights/Xception_finetuned.h5', force_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-04-26T19:27:08.956Z"
    }
   },
   "outputs": [],
   "source": [
    "image_id=39\n",
    "with tf.device('/cpu:0'):\n",
    "    Xception.show_heatmap_prediction(image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-26T18:58:57.106590Z",
     "start_time": "2021-04-26T18:58:57.090366Z"
    }
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>ResNet50</b> <br>\n",
    "We will do the same as previously done for Xception. But now with a bigger base model which is resnet50. This model is commonly used as feature extractor in popular models. \n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Model(ClassifactionModel): \n",
    "    def __init__(self, config):\n",
    "        # setup model name for wandb\n",
    "        self.project_name = 'resnet50'\n",
    "        self.heatmap_layer_name='conv5_block3_out'\n",
    "        # define the base model\n",
    "        self.base_model = keras.applications.ResNet50V2(weights=\"imagenet\",\n",
    "                                                               include_top=False)\n",
    "        # super takes care of the rest\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # feed preprocessor function \n",
    "        self.dataset.feed_preprocess_function(keras.applications.resnet_v2.preprocess_input)\n",
    "config = {\n",
    "    'name': 'ResNet50',\n",
    "    'logging_wandb': False,  #nice tool for tracking a run. make and account on wandb.ai and I will add you to this project\n",
    "    'weights': \"imagenet\", # 'imagenet', #None, \n",
    "    'nbr_classes': 20,\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_base_model': False, # whether to train the head and or base model\n",
    "    'train_head_model': True, \n",
    "    'train_parameters': {\n",
    "        'optimizer': 'ADAM',\n",
    "        'epochs': 3,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.000001, \n",
    "        'steps_per_epoch': 2000\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    },\n",
    "    'head_model': {\n",
    "        'head_model_units': [], \n",
    "        'add_dropout':      [],\n",
    "        'activation': 'relu',\n",
    "        'output_activation': 'softmax'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50=ResNet50Model(config)\n",
    "resnet50.prepare_for_inference('weights/resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id=6\n",
    "with tf.device('/cpu:0'):\n",
    "    resnet50.show_heatmap_prediction(image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypermodel tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>Finding the best hyperparameters</b> <br>\n",
    "At the start of the project we were chasing our tail a bit in finding good parameters for our head model. So as an experiment we wrote a hyperparameter tuner model to automatically find those best parameters.\n",
    "In the end, it turned out the problem was rather our base model giving us bad features to train on, which we will address with EfficientNet below.\n",
    "The implementation given here is more included as a stop on our journey, to demonstrate what we tried.\n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50HyperModel(kt.HyperModel):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def build(self, hp):\n",
    "        resnet50 = tf.keras.applications.ResNet50V2(include_top=False, weights='imagenet', input_shape=self.config['input_shape'])\n",
    "        resnet50.trainable = False\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(resnet50)\n",
    "        model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "        model.add(tf.keras.layers.BatchNormalization())\n",
    "        # add head model hidden layers set by hyperparams\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "                units=hp.Int('head_units_0',min_value=256,max_value=1024,step=64),\n",
    "                activation='relu',\n",
    "                kernel_regularizer='l2'))\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "                units=hp.Int('head_units_1',min_value=32,max_value=256,step=32),\n",
    "                activation='relu',\n",
    "                kernel_regularizer='l2'))\n",
    "        # add head model output layer with sigmoid activation\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "                units=self.config['nbr_classes'],\n",
    "                activation='sigmoid'))\n",
    "        self.model = model\n",
    "        # get optimizer from hyperparameters\n",
    "        lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n",
    "        optimizer = tf.keras.optimizers.SGD(lr)\n",
    "        # track some metrics while training\n",
    "        metrics = [\n",
    "                    tf.keras.metrics.BinaryAccuracy()\n",
    "                  , tfa.metrics.FBetaScore(num_classes=20, beta=2., average='weighted')\n",
    "                  ]\n",
    "#         loss = tfa.losses.SigmoidFocalCrossEntropy(reduction='auto')\n",
    "        loss = tf.keras.losses.KLDivergence()\n",
    "        self.model.compile(optimizer=optimizer, metrics=metrics, loss=loss)\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetHyperClassModel(RandomClassificationModel):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.hypermodel = ResNet50HyperModel(config['hypermodel'])\n",
    "        self.tuner = kt.Hyperband(\n",
    "            self.hypermodel,\n",
    "            objective=kt.Objective(\"val_fbeta_score\", direction=\"max\"),\n",
    "            max_epochs=20,\n",
    "            directory='hypertuners',\n",
    "            project_name=config['hypermodel']['wandb']['project']\n",
    "        )\n",
    "        \n",
    "    def load_best_model_from_disk(self):\n",
    "        self.best_model = self.tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, Dataset_Classification):\n",
    "            print(\"using custom dataset\")\n",
    "            self.tuner.search(\n",
    "                x = X.train_generator(self.config['train_parameters']['batch_size']), \n",
    "                validation_data = X.validation_generator(self.config['train_parameters']['batch_size']),\n",
    "                **self.config['tuner_search']\n",
    "            )\n",
    "        else:\n",
    "            self.tuner.search(X, y, validation_split=.2, **self.config['tuner_search'])\n",
    "        self.best_model = self.tuner.get_best_models(num_models=1)[0]\n",
    "            \n",
    "    def predict(self, X):\n",
    "        # let best model predict\n",
    "        if isinstance(X, Dataset_Classification):\n",
    "            pred = self.best_model.predict(x=X.test_generator(50), use_multiprocessing=True)\n",
    "        else:\n",
    "            pred = self.best_model.predict(X)\n",
    "        # threshold output\n",
    "        pred[pred >= .5] = 1\n",
    "        pred[pred <  .5] = 0\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    },\n",
    "    'train_parameters': {\n",
    "        'batch_size': 64\n",
    "    },\n",
    "    'hypermodel': {\n",
    "        'input_shape': (224, 224, 3),\n",
    "        'nbr_classes': 20\n",
    "    },\n",
    "    'tuner_search': {\n",
    "        'steps_per_epoch': 256,\n",
    "        'validation_steps': 32\n",
    "    }\n",
    "}\n",
    "hyper_resnet = ResNetHyperClassModel(config)\n",
    "hyper_dataset = Dataset_Classification(hyper_resnet.config['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform hyperparameter tuning on model, automatically writes all runs to disk\n",
    "hyper_resnet.fit(hyper_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best model so far"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>EfficientNet</b> <br>\n",
    "The best model for classification we have found is listed below. This model is way better than the ones before for the following reasons:\n",
    "<ol>\n",
    "  <li><b>Base model:</b> garbage in garbage out is something of an unofficial motto for machine learning. We began to suspect our base model was not giving us good features to learn on, which means the head model cannot be expected to produce good results. Switching to efficientnet (pretrained on imagenet) instantly produced a better model (in combination with the tweaks listed below).</li>\n",
    "  <li><b>Output activation:</b> in the above models, we also made a mistake in our output activation. Choosing softmax forces the sum of the output vector to be one. When the input image needs just one label, this is not a problem. Some images get multiple labels though, leading to confusion for the model and becoming a tossup between the two correct labels. Using sigmoid activation fixes this, by making the output neurons independent from eachother. Using this output roughly corresponds to creating 20 independent 1 neuron softmax outputs, classifying between \"yes this label\" and \"not this label\".</li>\n",
    "  <li><b>Loss:</b> the dataset is inherently very biased towards classifying 0 for any output label. This is because when, for example for the label bicycle 5% of the images contain a bicycle, 95% of all images will not. Bias like this is less of a problem in softmax, because the output activation in itself shows strong bias towards classifying at least one thing to 1. Sigmoid activation does not have this advantage, so it just classifies everything to zero standard. Focal loss can help here, which was introduced for dense object detection problems. Our problem is very analogous, because we have to select one or two from a (relatively) long list of possible classes. Introducing this loss solved the problem of all-zero predictions, and boosted our score the most obviously. Using a loss which does not steer training towards better accuracy makes training pointless.\n",
    "Focal loss penalizes giving no label more than giving a wrong label. How much more exactly can be tuned by parameres alpha and gamma, but we found the defaults fit our task well.</li>\n",
    "<li><b>Accuracy metric:</b> of course, an accuracy metric is also important to track the progress of training towards our goal. F-Beta is a logical pairing with focal loss, as it also can be calibrated to penalize no label more. With F-Beta we talk about prioritizing recall above precision. It takes a harmonic mean of these two metrics, with a beta > 1 giving more weight to recall. We set beta to 2 and obtained a metric which tracked well with the score we got from kaggle.</li>\n",
    "</ol>\n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetModel(ClassifactionModel):\n",
    "    def __init__(self, config):\n",
    "        # setup model name for wandb\n",
    "        self.project_name = 'efficientnet'\n",
    "        self.heatmap_layer_name='top_conv'\n",
    "        # define the base model\n",
    "        self.base_model = tf.keras.applications.EfficientNetB0(include_top=False, weights='imagenet')\n",
    "        # super takes care of the rest\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # feed preprocessor function \n",
    "        self.dataset.feed_preprocess_function(tf.keras.applications.efficientnet.preprocess_input)\n",
    "config = {\n",
    "    'name': 'efficientnet',\n",
    "    'logging_wandb': False,  #nice tool for tracking a run. make and account on wandb.ai and I will add you to this project\n",
    "    'weights': \"imagenet\", # 'imagenet', #None, \n",
    "    'nbr_classes': 20,\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_base_model': False, # whether to train the head and or base model\n",
    "    'train_head_model': True, \n",
    "    'train_parameters': {\n",
    "        'optimizer': 'ADAM',\n",
    "        'epochs': 3,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.0001, \n",
    "        'steps_per_epoch': 2000,\n",
    "        'loss': 'focal'\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    },\n",
    "    'head_model': {\n",
    "        'head_model_units': [256], \n",
    "        'add_dropout':      [False],\n",
    "        'activation': 'relu',\n",
    "        'output_activation': 'sigmoid'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effNetModel = EfficientNetModel(config)\n",
    "effNetModel.train(None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pred = effNetModel.model.predict(x=effNetModel.dataset.test_generator(50), use_multiprocessing=True)\n",
    "clf_pred[clf_pred >= .5] = 1\n",
    "clf_pred[clf_pred <  .5] = 0\n",
    "test_df.iloc[:,0:20] = clf_pred.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_test_classes(image_id):\n",
    "    fig, ax = plt.subplots()\n",
    "    real_image = np.load('data/test/img/test_{}.npy'.format(image_id) )\n",
    "    plt.suptitle('Classes: {}'.format(test_df.iloc[:,0:20].columns[clf_pred[image_id]==1.].tolist()))\n",
    "    ax.imshow(real_image)\n",
    "    fig.show()\n",
    "image_id = ipywidgets.IntText(min=0, max=749, value=0, label='image_id')\n",
    "interact(show_test_classes,  image_id=image_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(ResNetClassifactionModel): \n",
    "    def __init__(self, config):\n",
    "        self.config = config \n",
    "        self.config_head = config['head_model']\n",
    "        self.project_name = 'custom'\n",
    "        \n",
    "        # initialize dataset\n",
    "        self.dataset = Dataset_Classification(config['dataset'])\n",
    "        \n",
    "        # feed preprocessor to dataset\n",
    "        print('Feeding resnet50 preprocess function to dataset class')\n",
    "        self.dataset.feed_preprocess_function(lambda x: x/255.0)\n",
    "    \n",
    "    def build(self):\n",
    "        print('simple cnn')\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Conv2D(64,7, activation='relu', padding=\"same\", \n",
    "                                input_shape=self.config['input_shape']), \n",
    "            keras.layers.MaxPooling2D(2), \n",
    "            keras.layers.Conv2D(128,3,activation='relu', padding='same'),\n",
    "            keras.layers.Conv2D(128,3,activation='relu', padding='same'),\n",
    "            keras.layers.MaxPooling2D(2),\n",
    "            keras.layers.Conv2D(256,3,activation='relu', padding='same'),\n",
    "            keras.layers.Conv2D(256,3,activation='relu', padding='same'),\n",
    "            keras.layers.MaxPooling2D(2), \n",
    "\n",
    "            keras.layers.Flatten(), \n",
    "            keras.layers.Dense(128, activation='relu'), \n",
    "            keras.layers.Dropout(0.5), \n",
    "            keras.layers.Dense(64, activation='relu'), \n",
    "            keras.layers.Dropout(0.5), \n",
    "            keras.layers.Dense(self.config['nbr_classes'], activation='softmax')\n",
    "            ])\n",
    "        self.model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'name': 'resnet50 KL divergence',\n",
    "    'logging_wandb': False,  #nice tool for tracking a run. make and account on wandb.ai and I will add you to this project\n",
    "    'weights': 'imagenet', #None, \n",
    "    'nbr_classes': 20,\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_base_model': True, # whether to train the head and or base model\n",
    "    'train_head_model': True, \n",
    "    'train_parameters': {\n",
    "        'epochs': 20,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.01, \n",
    "        'steps_per_epoch': 100\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        \"uniform_sample_probabilities\": False\n",
    "    },\n",
    "    'head_model': {\n",
    "        'head_model_units': [1024, 1024], \n",
    "        'add_dropout':      [True, True],\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Custom = CustomModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_run=''\n",
    "notes=''\n",
    "tags = ['custom']\n",
    "Custom.train(name_run, notes, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19ClassifactionModel(ResNetClassifactionModel): \n",
    "    def __init__(self, config): \n",
    "        self.config = config \n",
    "        self.config_head = config['head_model']\n",
    "        \n",
    "        self.project_name = 'VGG19'\n",
    "        \n",
    "        # initialize dataset\n",
    "        self.dataset = Dataset_Classification(config['dataset'])\n",
    "        \n",
    "        # feed preprocessor to dataset\n",
    "        print('Feeding vgg19 preprocess function to dataset class')\n",
    "        self.dataset.feed_preprocess_function(tf.keras.applications.vgg19.preprocess_input)\n",
    "        \n",
    "        # check if some configurations make sense \n",
    "        assert len(self.config_head['head_model_units']) == len(self.config_head['add_dropout']), 'head_models_units and add_dropout list should have same size'\n",
    "        \n",
    "    def build(self): \n",
    "        \"\"\"\n",
    "            Builds the model \n",
    "        \"\"\"\n",
    "        # define a resnet50 base model\n",
    "        VGG19 = tf.keras.applications.VGG19(\n",
    "                    include_top=False,\n",
    "                    weights=self.config['weights'],\n",
    "                    input_shape=self.config['input_shape'],\n",
    "                        )\n",
    "    \n",
    "        self.base_model = VGG19\n",
    "        \n",
    "        # define a head model\n",
    "        head_model=VGG19.output\n",
    "        #head_model=tf.keras.layers.AveragePooling2D(pool_size=(7,7))(head_model)\n",
    "        head_model=tf.keras.layers.Flatten()(head_model)\n",
    "        \n",
    "        for (nbr_units, dropout) in zip(self.config_head['head_model_units'], self.config_head['add_dropout']): \n",
    "            head_model=tf.keras.layers.Dense(nbr_units, activation=self.config_head['activation'])(head_model)\n",
    "            if dropout:\n",
    "                head_model=tf.keras.layers.Dropout(0.2)(head_model)\n",
    "        \n",
    "        head_model=tf.keras.layers.Dense(self.config['nbr_classes'], activation='linear')(head_model)\n",
    "        \n",
    "        self.head_model = head_model\n",
    "                \n",
    "            \n",
    "        # combine both models \n",
    "        self.model = tf.keras.Model(self.base_model.input, self.head_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'name': 'vgg19',\n",
    "    'logging_wandb': False, \n",
    "    'weights': None, \n",
    "    'nbr_classes': 20,\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_base_model': True, # whether to train the head and or base model\n",
    "    'train_head_model': True, \n",
    "    'train_parameters': {\n",
    "        'epochs': 10,\n",
    "        'batch_size': 32,\n",
    "        'learning_rate': 0.001, \n",
    "        'steps_per_epoch': 100\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    },\n",
    "    'head_model': {\n",
    "        'head_model_units': [2048], \n",
    "        'add_dropout':      [False],\n",
    "        'activation': 'linear'\n",
    "    }\n",
    "}\n",
    "#64, 128, 256, 512, 1024, 2048, 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model=VGG19ClassifactionModel(config)\n",
    "# vgg_model.build()\n",
    "# vgg_model.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_run='vgg19_'\n",
    "notes='First try for finetuning vgg19 on pretrained imagenet weights. Data augmentation turned on. '\n",
    "tags = ['resnet50', 'head = [2048]', 'head = [False]', 'Augmentation applied']\n",
    "vgg_model.train(name_run, notes, tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    'train_fraction': 0.9,\n",
    "    'input_shape': (224, 224),\n",
    "    'augmentation': False,\n",
    "    'uniform_sample_probabilities': False\n",
    "}\n",
    "ds = Dataset_Segmentation(dataset_config)\n",
    "# resnet50 preprocessor \n",
    "ds.feed_preprocess_function(lambda x: x/255.0)\n",
    "\n",
    "i = 0\n",
    "for X,y in ds.train_generator(0): \n",
    "    if i > 1: \n",
    "        break \n",
    "    i+=1 \n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].imshow(X[0])\n",
    "axes[1].imshow(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y[0].shape)\n",
    "print(np.unique(y[0]))\n",
    "idx = np.where(y[0] == 2)\n",
    "zrs = np.zeros(y[0].shape)\n",
    "zrs[idx]=1\n",
    "plt.imshow(zrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ds.test_generator(1)\n",
    "im = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "model=tf.keras.applications.ResNet50V2()\n",
    "\n",
    "# predict \n",
    "preds = model.predict(gen)\n",
    "#preds = model.predict(im_pr)\n",
    "tf.keras.applications.imagenet_utils.decode_predictions(\n",
    "    preds, top=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.194695,
     "end_time": "2021-03-29T09:05:22.154564",
     "exception": false,
     "start_time": "2021-03-29T09:05:21.959869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Semantic segmentation\n",
    "The goal here is to implement a segmentation CNN that labels every pixel in the image as belonging to one of the 20 classes (and/or background). Use the training set to train your CNN and compete on the test set (by filling in the segmentation column in the test dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T09:05:22.577753Z",
     "iopub.status.busy": "2021-03-29T09:05:22.576727Z",
     "iopub.status.idle": "2021-03-29T09:05:34.965517Z",
     "shell.execute_reply": "2021-03-29T09:05:34.966002Z"
    },
    "papermill": {
     "duration": 12.607971,
     "end_time": "2021-03-29T09:05:34.966182",
     "exception": false,
     "start_time": "2021-03-29T09:05:22.358211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomSegmentationModel:\n",
    "    \"\"\"\n",
    "    Random segmentation model: \n",
    "        - generates random label maps for the inputs based on the class distributions observed during training\n",
    "        - every pixel in an input can only have one label\n",
    "    \"\"\"\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in Y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        Y: list of arrays - n x (height x width)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean([[np.sum(Y_ == i) / Y_.size for i in range(len(labels) + 1)] for Y_ in Y], axis=0)\n",
    "        print(\"Setting class distribution to:\\nbackground: {}\\n{}\".format(self.distribution[0], \"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution[1:]))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label map.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "             \n",
    "        Returns\n",
    "        -------\n",
    "        Y_pred: list of arrays - n x (height x width)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.random.choice(np.arange(len(labels) + 1), size=X_.shape[:2], p=self.distribution) for X_ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "model = RandomSegmentationModel()\n",
    "model.fit(train_df[\"img\"], train_df[\"seg\"])\n",
    "test_df.loc[:, \"seg\"] = model.predict(test_df[\"img\"])\n",
    "test_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetSegmentationMode(ResNetClassifactionModel): \n",
    "    def __init__(self, config): \n",
    "        self.config=config\n",
    "        self.config_head = config['head_model']\n",
    "        self.output_channels=self.config['nbr_classes'] + 1 # plus one since background is a class\n",
    "        \n",
    "        # initialize dataset\n",
    "        self.dataset = Dataset_Segmentation(config['dataset'])\n",
    "        \n",
    "        # feed preprocessor to dataset\n",
    "        print('Feeding vgg19 preprocess function to dataset class')\n",
    "        self.dataset.feed_preprocess_function(tf.keras.applications.vgg19.preprocess_input)\n",
    "        \n",
    "        self.project_name = 'Unet'\n",
    "        \n",
    "        \n",
    "    ## \n",
    "    def upsample(self, filters, size, apply_dropout=False):\n",
    "        initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "        result = tf.keras.Sequential()\n",
    "        result.add(\n",
    "        tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                        padding='same',\n",
    "                                        kernel_initializer=initializer,\n",
    "                                        use_bias=False))\n",
    "\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "        if apply_dropout:\n",
    "            result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "        result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def compile_model(self): \n",
    "        # optimizer\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "                                learning_rate=self.config['train_parameters']['learning_rate'],\n",
    "                                beta_1=0.9,\n",
    "                                beta_2=0.999,\n",
    "                                epsilon=1e-07,\n",
    "                                amsgrad=False,\n",
    "                                name=\"Adam\",\n",
    "                            )\n",
    "\n",
    "        metrics = ['acc', \n",
    "                  tf.keras.metrics.SparseTopKCategoricalAccuracy(k=3, name='top 3 categorical acccuracy'), \n",
    "                  tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top 5 categorical acccuracy')\n",
    "                  ]\n",
    "\n",
    "\n",
    "        loss=loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        \n",
    "    def build(self):\n",
    "        ## base model\n",
    "        base_model = tf.keras.applications.MobileNetV2(input_shape=self.config['input_shape'], include_top=False)\n",
    "\n",
    "        # Use the activations of these layers\n",
    "        layer_names = [\n",
    "            'block_1_expand_relu',   # 64x64\n",
    "            'block_3_expand_relu',   # 32x32\n",
    "            'block_6_expand_relu',   # 16x16\n",
    "            'block_13_expand_relu',  # 8x8\n",
    "            'block_16_project',      # 4x4\n",
    "        ]\n",
    "        base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "        # Create the feature extraction model\n",
    "        down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "        down_stack.trainable = False # hard code it to be not trainable\n",
    "        \n",
    "        self.base_model = down_stack\n",
    "        ## head model\n",
    "        \n",
    "        up_stack = [\n",
    "            self.upsample(512, 3, self.config_head['add_dropout']),  # 4x4 -> 8x8\n",
    "            self.upsample(256, 3, self.config_head['add_dropout']),  # 8x8 -> 16x16\n",
    "            self.upsample(128, 3, self.config_head['add_dropout']),  # 16x16 -> 32x32\n",
    "            self.upsample(64, 3, self.config_head['add_dropout']),   # 32x32 -> 64x64\n",
    "        ]\n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=self.config['input_shape'])\n",
    "\n",
    "        # Downsampling through the model\n",
    "        skips = down_stack(inputs)\n",
    "        x = skips[-1]\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        # Upsampling and establishing the skip connections\n",
    "        for up, skip in zip(up_stack, skips):\n",
    "            x = up(x)\n",
    "            concat = tf.keras.layers.Concatenate()\n",
    "            x = concat([x, skip])\n",
    "\n",
    "        # This is the last layer of the model\n",
    "        last = tf.keras.layers.Conv2DTranspose(\n",
    "            self.output_channels, 3, strides=2,\n",
    "            padding='same')  #64x64 -> 128x128\n",
    "\n",
    "        x = last(x)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_config = {\n",
    "    'name': 'Unet',\n",
    "    'logging_wandb': True, \n",
    "    'nbr_classes': 20,\n",
    "    'train_base_model': False, # whether to train the head and or base model\n",
    "    'train_head_model': True, # not defined here\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_parameters': {\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.001, \n",
    "        'steps_per_epoch': 1000\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': True\n",
    "    },\n",
    "    'head_model': {\n",
    "        'add_dropout': True,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet = UnetSegmentationMode(Unet_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_run='Unet 1'\n",
    "notes='Changed configurations, higher batch size, more steps per epoch'\n",
    "tags = ['unet', 'Augmentation applied', 'dropout added']\n",
    "Unet.train(name_run, notes, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_seg = np.argmax(Unet.model.predict(x=Unet.dataset.test_generator(50)), axis=3).astype('uint8')\n",
    "test_df[\"seg\"] = [cv2.resize(pred_seg[i], test_df[\"seg\"][i].shape) for i in range(test_df.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.193218,
     "end_time": "2021-03-29T09:05:35.361389",
     "exception": false,
     "start_time": "2021-03-29T09:05:35.168171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submit to competition\n",
    "You don't need to edit this section. Just use it at the right position in the notebook. See the definition of this function in Sect. 1.3 for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T09:05:35.750969Z",
     "iopub.status.busy": "2021-03-29T09:05:35.750253Z",
     "iopub.status.idle": "2021-03-29T09:07:06.152716Z",
     "shell.execute_reply": "2021-03-29T09:07:06.153236Z"
    },
    "papermill": {
     "duration": 90.600773,
     "end_time": "2021-03-29T09:07:06.153425",
     "exception": false,
     "start_time": "2021-03-29T09:05:35.552652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_submission(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.197143,
     "end_time": "2021-03-29T09:07:06.546217",
     "exception": false,
     "start_time": "2021-03-29T09:07:06.349074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Adversarial attacks\n",
    "For this part, your goal is to fool your classification and/or segmentation CNN, using an *adversarial attack*. More specifically, the goal is build a CNN to perturb test images in a way that (i) they look unperturbed to humans; but (ii) the CNN classifies/segments these images in line with the perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task definition (we should come up with a better task definition)!!!!\n",
    "Our goal is to fool the network so that a well classified image gets classified with the wrong label. (No targeted attacks)\n",
    "\n",
    "## Methods\n",
    "There are probably two methods that will work here\n",
    "1. Fast Gradient Sign Method (FGSM)\n",
    "2. Projected Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Use best model for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    },\n",
    "    'train_parameters': {\n",
    "        'batch_size': 64\n",
    "    },\n",
    "    'hypermodel': {\n",
    "        'input_shape': (224, 224, 3),\n",
    "        'nbr_classes': 20,\n",
    "        'wandb': {\n",
    "            'project': 'resnethyper_new_focal',\n",
    "            'entity': 'cv-task-2',\n",
    "            'notes': 'ResNet50 being hyperparameter tuned with keras hyperband',\n",
    "            'tags': ['resnet50', 'hyperband']\n",
    "        }\n",
    "    },\n",
    "    'tuner_search': {\n",
    "        'steps_per_epoch': 256,\n",
    "        'validation_steps': 32\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(224,224,3))\n",
    "efficientnet = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False, weights='imagenet', input_tensor=inputs\n",
    ")\n",
    "efficientnet.trainable = False\n",
    "model = tf.keras.Sequential()\n",
    "model.add(efficientnet)\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer='l2'))\n",
    "model.add(tf.keras.layers.Dense(20, activation='sigmoid'))#, kernel_regularizer='l2'))\n",
    "model.compile(loss=tfa.losses.SigmoidFocalCrossEntropy(reduction='auto'),\n",
    "              optimizer='adam',\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(), tfa.metrics.FBetaScore(num_classes=20, beta=2., average='weighted')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset_Classification(config['dataset'])\n",
    "model = keras.models.load_model('models/efficientnetb0_class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGSM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (8, 8)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = tf.image.resize(image, (224, 224))\n",
    "  image = image[None, ...]\n",
    "  return image\n",
    "\n",
    "def get_labels_prob(image_probs, label_names):\n",
    "    clf_pred = np.array(np.copy(image_probs))\n",
    "    clf_pred[image_probs >= .5] = 1\n",
    "    clf_pred[image_probs <  .5] = 0\n",
    "    return label_names[np.where(clf_pred > 0.9)[1]],  image_probs[image_probs >= .5]\n",
    "\n",
    "def get_top_n_labels(image_probs, label_names, n):\n",
    "    clf_pred = np.array(np.copy(image_probs))\n",
    "    sorted_index = np.argsort(clf_pred)\n",
    "    return clf_pred[0][sorted_index[0][-n:]], label_names[sorted_index[0][-n:]]\n",
    "\n",
    "def get_multiple_hot_encoding(image_probs):\n",
    "    clf_pred = np.array(np.copy(image_probs))\n",
    "    clf_pred[image_probs >= .5] = 1\n",
    "    clf_pred[image_probs <  .5] = 0\n",
    "    return clf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = ds.test_generator(1)\n",
    "image = next(test_images)\n",
    "image = image.reshape(224,224,3)\n",
    "image = preprocess(image)\n",
    "image_probs = model.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = get_labels_prob(image_probs, ds.label_names)\n",
    "plt.imshow(tf.reshape(image,(224,224,3))/255)\n",
    "title_string = [str(i) + \" \" + \"{:.2f}%\".format(j*100) for i,j in zip(x,y)]\n",
    "_ = plt.title(\" \".join(title_string))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tfa.losses.SigmoidFocalCrossEntropy(reduction='auto')\n",
    "\n",
    "def create_adversarial_pattern(input_image, input_labels): \n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(input_image)\n",
    "    prediction = model(input_image) # this function only works on uints images\n",
    "    loss = loss_object(input_labels, prediction)\n",
    "\n",
    "  # Get the gradients of the loss w.r.t to the input image.\n",
    "  gradient = tape.gradient(loss, input_image)\n",
    "  # Get the sign of the gradients to create the perturbation\n",
    "  signed_grad = tf.sign(gradient)\n",
    "  return signed_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_labels = tf.convert_to_tensor(get_multiple_hot_encoding(image_probs))\n",
    "perturbations = create_adversarial_pattern(image, input_labels)\n",
    "plt.imshow(perturbations[0] * 0.5 + 0.5);  # To change [-1, 1] to [0,1]\n",
    "_ = plt.title(\"perturbations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(image, description):\n",
    "    plt.figure()\n",
    "    image_probs = model.predict(image)\n",
    "    x,y = get_top_n_labels(image_probs, ds.label_names, 3)\n",
    "    title_string = [str(i) + \" \" + \"{:.2f}%\".format(j*100) for j,i in zip(x,y)]\n",
    "    plt.title(\" \".join(title_string))\n",
    "    plt.imshow(tf.reshape(image, (224,224,3))/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = [0, 0.1, 1, 5, 10, 20]\n",
    "descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')\n",
    "                for eps in epsilons]\n",
    "\n",
    "for i, eps in enumerate(epsilons):\n",
    "  adv_x = image + eps*perturbations\n",
    "  display_images(adv_x, descriptions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO with FGSM (run previous cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset_Classification(config['dataset'])\n",
    "model = keras.models.load_model('models/efficientnetb0_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_show_adverserial_image(index, epsilon, n_pred):\n",
    "    test_images = ds.test_generator(1)\n",
    "    image = next(test_images)\n",
    "    for i in range(index):\n",
    "        image = next(test_images)\n",
    "    image = image.reshape(224,224,3)\n",
    "    image = preprocess(image)\n",
    "    image_probs = model.predict(image)\n",
    "    input_labels = tf.convert_to_tensor(get_multiple_hot_encoding(image_probs))\n",
    "    perturbations = create_adversarial_pattern(image, input_labels)\n",
    "    adv_x = image + epsilon*perturbations\n",
    "    display_attack(image, perturbations, adv_x, epsilon)\n",
    "\n",
    "def display_attack(image, perturbation, adv_x, epsilon):\n",
    "    x1,y1 = get_top_n_labels(model.predict(image), ds.label_names, 3)\n",
    "    title_string_im = [str(i) + \" \" + \"{:.2f}%\".format(j*100) for j,i in zip(x1,y1)]\n",
    "    x2,y2 = get_top_n_labels(model.predict(adv_x), ds.label_names, 3)\n",
    "    title_string_adv = [str(i) + \" \" + \"{:.2f}%\".format(j*100) for j,i in zip(x2,y2)]\n",
    "    f, axarr = plt.subplots(1,3, figsize=(21,21))        \n",
    "    axarr[0].imshow(tf.reshape(image, (224,224,3))/255)\n",
    "    axarr[0].set_title(\" \".join(title_string_im))\n",
    "    axarr[1].imshow(perturbation[0] * 0.5 + 0.5)\n",
    "    axarr[1].set_title(\"perturbation*\" + str(epsilon))\n",
    "    axarr[2].imshow(tf.reshape(adv_x, (224,224,3))/255)\n",
    "    axarr[2].set_title(\" \".join(title_string_adv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ipywidgets.IntText(min=0, max=ds.get_test_set_size(), value=0, label='image_id')\n",
    "p_multiplier = ipywidgets.FloatText(min=0, max=100, value=5, label='perturbation multiplier')\n",
    "top_n = ipywidgets.IntText(min=0, max=5, value=3, label='top_n_labels')\n",
    "interact(create_and_show_adverserial_image, index=index, epsilon=p_multiplier, n_pred=top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other methods\n",
    "Here we will try to make targeted attacks with pgd\n",
    "targeted attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.195133,
     "end_time": "2021-03-29T09:07:06.938031",
     "exception": false,
     "start_time": "2021-03-29T09:07:06.742898",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Discussion\n",
    "Finally, take some time to reflect on what you have learned during this assignment. Reflect and produce an overall discussion with links to the lectures and \"real world\" computer vision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 178.155457,
   "end_time": "2021-03-29T09:07:09.396364",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-29T09:04:11.240907",
   "version": "2.2.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
