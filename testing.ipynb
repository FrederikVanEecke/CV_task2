{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752ae761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os \n",
    "import multiprocessing\n",
    "import wandb\n",
    "# !pip install wandb -qqq\n",
    "from wandb.keras import WandbCallback\n",
    "import kerastuner as kt #!python3.x -m pip install keras-tuner\n",
    "import cv2\n",
    "from ipywidgets import fixed, interact \n",
    "import ipywidgets\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
    "    RandomBrightness, RandomContrast, RandomGamma,\n",
    "    ToFloat, ShiftScaleRotate, RandomBrightnessContrast, RandomCrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925a4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Classification(object): \n",
    "    \"\"\"\n",
    "     A dataset class usefull when training a classification model. \n",
    "    \"\"\"\n",
    "    def __init__(self, config): \n",
    "        self.config = config\n",
    "        \n",
    "        # set labels in a usefull format\n",
    "        self._classification_labels()\n",
    "        \n",
    "        # initialize\n",
    "        self.initialize()\n",
    "        \n",
    "        # to keep track of sampling\n",
    "        self.sampling_check = np.zeros(20)\n",
    "        self.times_sampled = 0\n",
    "        \n",
    "        \n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "            Performs necessary thingss\n",
    "        \"\"\"\n",
    "        self.train_data_path = 'data/train/img'\n",
    "        self.test_data_path = 'data/test/img'\n",
    "        \n",
    "        # count nbr of files within data set. \n",
    "        self.nbr_of_train_images = len(os.listdir(self.train_data_path))\n",
    "        self.nbr_of_test_images = len(os.listdir(self.test_data_path))\n",
    "        \n",
    "        # prepare train/validation split \n",
    "        train_fraction = self.config['train_fraction']\n",
    "        r_idx=np.random.permutation(self.nbr_of_train_images)\n",
    "        \n",
    "        self.train_indices = r_idx[:int(train_fraction*self.nbr_of_train_images)]\n",
    "        self.train_sample_probs = self.probabilities[self.train_indices]/np.sum(self.probabilities[self.train_indices])\n",
    "        \n",
    "        self.validation_indices = r_idx[int(train_fraction*self.nbr_of_train_images):]\n",
    "        self.validation_sample_probs = self.probabilities[self.validation_indices]/np.sum(self.probabilities[self.validation_indices])\n",
    "        \n",
    "        print('Found {} train images'.format(self.nbr_of_train_images))\n",
    "        print('- {} used for training, {} used for validating'.format(len(self.train_indices), len(self.validation_indices)))\n",
    "        print('Found {} test images'.format(self.nbr_of_test_images))\n",
    "        \n",
    "        if self.config['augmentation']: \n",
    "            print('Including augmentation when training data is generated')\n",
    "        self.augment = Compose([\n",
    "                        #RandomCrop(width=self.config['input_shape'][0], height=self.config['input_shape'][0]),\n",
    "                        HorizontalFlip(p=0.5),\n",
    "                        RandomContrast(limit=0.1,p=0.25),\n",
    "                        #RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
    "                        RandomBrightness(limit=0.15, p=0.5),\n",
    "#                         HueSaturationValue(hue_shift_limit=1.5, sat_shift_limit=5,\n",
    "#                                            val_shift_limit=2.5, p=.7),\n",
    "                        # CLAHE(p=1.0, clip_limit=2.0),\n",
    "                        ShiftScaleRotate(\n",
    "                            shift_limit=0.1, scale_limit=0.1, \n",
    "                            rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), \n",
    "                    ])\n",
    "    \n",
    "        self.preprocessor = lambda x: x #defualt preprocessor does nothing \n",
    "        \n",
    "    def get_test_set_size(self): \n",
    "        return self.nbr_of_test_images\n",
    "    def get_train_set_size(self): \n",
    "        return self.nbr_of_train_images\n",
    "        \n",
    "    def reshape(self,im): \n",
    "        return cv2.resize(im, self.config['input_shape'])\n",
    "        \n",
    "    def _classification_labels(self): \n",
    "        \"\"\"\n",
    "            Get the classification labels \n",
    "        \"\"\"\n",
    "        # label names \n",
    "        train_df = pd.read_csv('data/train/train_set.csv', index_col=\"Id\")\n",
    "        self.label_names = train_df.columns.to_numpy()\n",
    "        \n",
    "        # get rid of pandas frame \n",
    "        self.labels = train_df.to_numpy() # each row corresponds to label\n",
    "        \n",
    "        # instances per class\n",
    "        absolute_nbr_of_instances_per_class = np.sum(self.labels,axis=0)\n",
    "        # get array with label name for each image \n",
    "        self.class_name_per_image = list()\n",
    "        probabilities = list()\n",
    "        total_class_prob = 1/20*np.ones(20)\n",
    "        #total_class_prob[14]=0\n",
    "        for row in self.labels: #can probably be done more eligant\n",
    "            idx = np.where(row==1)[0]\n",
    "            if 14 in idx:\n",
    "                idx=14\n",
    "            else:\n",
    "                idx=idx[0]\n",
    "            probabilities.append(total_class_prob[idx]/absolute_nbr_of_instances_per_class[idx])\n",
    "            self.class_name_per_image.append(self.label_names[idx])\n",
    "        \n",
    "        \n",
    "        # make sure total probability sums to 1\n",
    "        if self.config['uniform_sample_probabilities']:\n",
    "            self.probabilities = np.ones(np.array(probabilities).shape)/len(probabilities) # uniform sampling\n",
    "        else:\n",
    "            self.probabilities = np.array(probabilities)/np.sum(probabilities) # sampling based on distribution of classes in trainning data\n",
    "            \n",
    "            \n",
    "    def feed_preprocess_function(self, preprocessor): \n",
    "        \"\"\"\n",
    "            Each network needs it's batches preprocessed in some manner. Feed this function to the Dataset object \n",
    "            who will call it when asking for batches.\n",
    "            \n",
    "            The preprocessor takes \n",
    "        \"\"\"\n",
    "        self.preprocessor = preprocessor\n",
    "    \n",
    "    def prepare_image(self, image): \n",
    "        \"\"\"\n",
    "            Function that performs all necessary steps from input image to image passed during trainnig. \n",
    "            This method should be overwritten depending on the model used.\n",
    "        \"\"\"\n",
    "        h,w,c=image.shape\n",
    "        # resize manually, when augmentation is turned on a random crop will be done 100% of times.\n",
    "        #if not self.config['augmentation'] or h < self.config['input_shape'][0]or w < self.config['input_shape'][1]:\n",
    "        image = self.reshape(image)\n",
    "\n",
    "        # augment if augmentation is turned on \n",
    "        if self.config['augmentation']:\n",
    "#             print('Augmentation enabled: check if combination of preprocessor and augmentation makes sence')\n",
    "            image=self.augment(image=image)[\"image\"]\n",
    "        \n",
    "        # preprocess \n",
    "        image = self.preprocessor(image)\n",
    "        return image\n",
    "    \n",
    "    def prepare_test_image(self, image): \n",
    "        \"\"\"\n",
    "             Same as prepare_image but without augmentation\n",
    "        \"\"\"\n",
    "        h,w,c=image.shape\n",
    "        # resize manually, when augmentation is turned on a random crop will be done 100% of times.\n",
    "        #if not self.config['augmentation'] or h < self.config['input_shape'][0]or w < self.config['input_shape'][1]:\n",
    "        image = self.reshape(image)\n",
    "        \n",
    "        # preprocess \n",
    "        image = self.preprocessor(image)\n",
    "        return image\n",
    "    \n",
    "    def view_preprocessed_image(self, image_id, option='train'): \n",
    "        \"\"\"\n",
    "            Shows an image as it is passed during training/testing of the network. \n",
    "            image_id \n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'preprocessor'), 'set a preprocessor function before using this.'\n",
    "        \n",
    "        # get image \n",
    "        if option=='train':\n",
    "            # get image \n",
    "            real_image = np.load('data/train/img/train_{}.npy'.format(image_id) )\n",
    "            label = self.class_name_per_image[image_id]\n",
    "        else: \n",
    "            real_image = np.load('data/test/img/test_{}.npy'.format(image_id) )\n",
    "            label = 'unknown'\n",
    "            \n",
    "        image=np.copy(real_image)\n",
    "        \n",
    "        image = self.prepare_image(image)\n",
    "        \n",
    "    \n",
    "    \n",
    "        # print some info \n",
    "        print('original image:')\n",
    "        print('-original_shape:', real_image.shape)\n",
    "        print('-dtype:', real_image.dtype)\n",
    "        print('-min value:', np.min(real_image))\n",
    "        print('-max value:', np.max(real_image))\n",
    "        \n",
    "        print('final image:')\n",
    "        print('-final shape:', image.shape)\n",
    "        print('-dtype:', image.dtype)\n",
    "        print('-min value:', np.min(image))\n",
    "        print('-max value:', np.max(image))\n",
    "        \n",
    "        \n",
    "        # show figure \n",
    "        fig, axes = plt.subplots(1,2, figsize=(30,15))\n",
    "        axes[0].imshow(real_image)\n",
    "        axes[0].set_title('Original image', fontsize=50)\n",
    "        \n",
    "        axes[1].imshow(image) # clip it to [0,1] range\n",
    "        axes[1].set_title('preprocessed_image', fontsize=50)\n",
    "        \n",
    "        plt.suptitle('label: {}'.format(label), fontsize=50)\n",
    "        fig.show()\n",
    "        \n",
    "    def view_possible_augmentations(self, image_id): \n",
    "        fig, axes = plt.subplots(4,4, figsize=(60,30))\n",
    "        real_image = np.load('data/train/img/train_{}.npy'.format(image_id) )\n",
    "        plt.suptitle('original image is on the top left', fontsize=50)\n",
    "        for ax in axes.flat: \n",
    "            ax.imshow(self.augment(image=real_image)[\"image\"])\n",
    "            ax.axis('off')\n",
    "        axes[0,0] = plt.imshow(real_image)\n",
    "        fig.show()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def train_generator(self,batch_size):\n",
    "        \"\"\"\n",
    "            generator that will feed training batches during training \n",
    "        \"\"\"\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        batchcount = 0\n",
    "        while True:\n",
    "#             for image_id in self.train_indices:\n",
    "            image_id = np.random.choice(self.train_indices, p=self.train_sample_probs)\n",
    "            # sample real image\n",
    "            real_image = np.load('data/train/img/train_{}.npy'.format(image_id) )\n",
    "\n",
    "            image = self.prepare_image(np.copy(real_image))\n",
    "            inputs.append(image)\n",
    "\n",
    "            # get corresponding label \n",
    "            targets.append(self.labels[image_id])\n",
    "            \n",
    "            self.sampling_check+=self.labels[image_id]\n",
    "            self.times_sampled+=1\n",
    "            \n",
    "\n",
    "            batchcount += 1\n",
    "            if batchcount >= batch_size:\n",
    "                X = np.array(inputs)\n",
    "                y = np.array(targets, dtype=np.uint8)\n",
    "                yield (X, y)\n",
    "                inputs = []\n",
    "                targets = []\n",
    "                batchcount = 0\n",
    "\n",
    "    def validation_generator(self,batch_size):\n",
    "        \"\"\"\n",
    "            generator that will feed validation batches during training \n",
    "        \"\"\"\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        batchcount = 0\n",
    "        while True:\n",
    "            #for image_id in self.validation_indices:\n",
    "            # sample real image\n",
    "            image_id = np.random.choice(self.validation_indices,p=self.validation_sample_probs)\n",
    "            real_image = np.load('data/train/img/train_{}.npy'.format(image_id) )\n",
    "\n",
    "            image = self.prepare_test_image(np.copy(real_image))\n",
    "\n",
    "            inputs.append(image)\n",
    "\n",
    "            # get corresponding label \n",
    "            targets.append(self.labels[image_id])\n",
    "\n",
    "            batchcount += 1\n",
    "            if batchcount >= batch_size:\n",
    "                X = np.array(inputs)\n",
    "                y = np.array(targets, dtype=np.uint8)\n",
    "                yield (X, y)\n",
    "                inputs = []\n",
    "                targets = []\n",
    "                batchcount = 0           \n",
    "    \n",
    "    def test_generator(self,batch_size):\n",
    "        \"\"\"\n",
    "            generator for feeding test data to model for prediction\n",
    "        \"\"\"\n",
    "        inputs = []\n",
    "        img_id = 0\n",
    "        while img_id < self.nbr_of_test_images:\n",
    "            raw_image = np.load('data/test/img/test_{}.npy'.format(img_id))\n",
    "            image = self.prepare_test_image(np.copy(raw_image))\n",
    "            inputs.append(image)\n",
    "            img_id += 1\n",
    "            if img_id%batch_size == 0:\n",
    "                X = np.array(inputs)\n",
    "                yield X\n",
    "                inputs = []\n",
    "        if len(inputs) > 0:\n",
    "            return np.array(inputs)\n",
    "    \n",
    "    def show_class_distribution(self): \n",
    "        fig,axes=plt.subplots(figsize=(30,15))\n",
    "        class_probs=np.mean(self.labels, axis=0)\n",
    "        axes.bar(self.label_names,  class_probs)\n",
    "        axes.tick_params(axis='both', which='major', labelsize=30)\n",
    "        for tick in axes.xaxis.get_major_ticks():\n",
    "            tick.label.set_rotation('vertical')\n",
    "        plt.suptitle('Class distribution within the training data.', fontsize=50)\n",
    "        fig.show()\n",
    "        \n",
    "    def show_training_sampling_distribution(self): \n",
    "        fig,axes=plt.subplots(figsize=(30,15))\n",
    "        class_probs=np.mean(self.labels, axis=0)\n",
    "        axes.bar(self.label_names,  self.sampling_check/self.times_sampled)\n",
    "        axes.tick_params(axis='both', which='major', labelsize=30)\n",
    "        for tick in axes.xaxis.get_major_ticks():\n",
    "            tick.label.set_rotation('vertical')\n",
    "        plt.suptitle('Number of times each class was sampled during training.', fontsize=50)\n",
    "        fig.show()\n",
    "    \n",
    "    def get_class_distribution(self):\n",
    "        return np.mean(self.labels, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18721e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    'train_fraction': 0.9,\n",
    "    'input_shape': (224, 224),\n",
    "    'augmentation': True, \n",
    "    'uniform_sample_probabilities': False\n",
    "}\n",
    "ds = Dataset_Classification(dataset_config)\n",
    "ds.show_class_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92dd888",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=0\n",
    "for (X,y) in ds.train_generator(10):\n",
    "    stop+=1\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    if stop>10: \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdf8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomClassificationModel:\n",
    "    \"\"\"\n",
    "    Random classification model: \n",
    "        - generates random labels for the inputs based on the class distribution observed during training\n",
    "        - assumes an input can have multiple labels\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        y: list of arrays - n x (nb_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean(y, axis=0)\n",
    "        print(\"Setting class distribution to:\\n{}\".format(\"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: list of arrays - n x (nb_classes)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.array([int(np.random.rand() < p) for p in self.distribution]) for _ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    \n",
    "class ClassifactionModel(RandomClassificationModel): \n",
    "    \"\"\"\n",
    "        Main class implementing all functions necessary to train and/or use a classification model \n",
    "        This class has to be overwritten for each specific model of interest, where the base model should be implemented.\n",
    "    \"\"\"\n",
    "    def __init__(self, config): \n",
    "        self.config = config \n",
    "        self.config_head = config['head_model']\n",
    "        \n",
    "        # initialize dataset\n",
    "        self.dataset = Dataset_Classification(config['dataset'])\n",
    "              \n",
    "        # check if some configurations make sense \n",
    "        assert len(self.config_head['head_model_units']) == len(self.config_head['add_dropout']), 'head_models_units and add_dropout list should have same size'\n",
    "    \n",
    "    \n",
    "    def set_config(self, config):\n",
    "        self.config = config \n",
    "        self.config_head = config['head_model']\n",
    "        self.dataset = Dataset_Classification(config['dataset'])\n",
    "        assert len(self.config_head['head_model_units']) == len(self.config_head['add_dropout']), 'head_models_units and add_dropout list should have same size'\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # \n",
    "        \n",
    "        if len(X.shape) == 1: \n",
    "            # X is a batch of images prepare all of them and create batch. \n",
    "            batch = np.array([self.dataset.prepare_test_image(im) for im in X])\n",
    "            y = model.predict(batch)\n",
    "        else: \n",
    "            # X is a single image \n",
    "            batch = self.dataset.prepare_test_image(X)\n",
    "            batch = np.expand_dims(batch, axis=0)\n",
    "            y = model.predict(batch)\n",
    "            \n",
    "        y = np.squeeze(y)\n",
    "\n",
    "        label_idx = np.where(y==1.)\n",
    "\n",
    "        return self.dataset.label_names[label_idx]\n",
    "            \n",
    "    def build(self): \n",
    "        \"\"\"\n",
    "            Builds the model \n",
    "        \"\"\"\n",
    "#       self.base_model = resnet50\n",
    "        \n",
    "        # define a head model\n",
    "        head_model=keras.layers.GlobalAveragePooling2D()(self.base_model.output)\n",
    "\n",
    "        for (nbr_units, dropout) in zip(self.config_head['head_model_units'], self.config_head['add_dropout']): \n",
    "            head_model=tf.keras.layers.Dense(nbr_units, activation=self.config_head['activation'])(head_model)\n",
    "            if dropout:\n",
    "                head_model=tf.keras.layers.Dropout(0.4)(head_model)\n",
    "        \n",
    "        head_model=keras.layers.Dense(20, activation='softmax')(head_model)\n",
    "        #self.config['nbr_classes']\n",
    "        self.head_model = head_model\n",
    "                  \n",
    "        # combine both models \n",
    "        self.model = keras.Model(self.base_model.input, head_model)\n",
    "\n",
    "\n",
    "#         avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "#         output = keras.layers.Dense(20, activation='softmax')(avg)\n",
    "#         model=keras.Model(inputs=base_model.input, outputs=output)\n",
    "           \n",
    "    def compile_model(self): \n",
    "        # optimizer\n",
    "        if self.config['train_parameters']['optimizer'] == 'SGD':\n",
    "            optimizer = tf.keras.optimizers.SGD(\n",
    "                    learning_rate=self.config['train_parameters']['learning_rate'], momentum=0.9,\n",
    "                    nesterov=False, name=\"SGD\"\n",
    "                )\n",
    "        elif self.config['train_parameters']['optimizer'] == 'ADAM':\n",
    "            optimizer = tf.keras.optimizers.Adam(lr=self.config['train_parameters']['learning_rate'])\n",
    "\n",
    "        # metric\n",
    "        metrics = [tf.keras.metrics.CategoricalAccuracy(),\n",
    "                  tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top 3 categorical acccuracy'), \n",
    "                  tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top 5 categorical acccuracy')\n",
    "                  ]\n",
    "        \n",
    "        # loss\n",
    "        loss='categorical_crossentropy'\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        \n",
    "        \n",
    "    def train(self, name_run, notes, tags):\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                # Currently, memory growth needs to be the same across GPUs\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "            except RuntimeError as e:\n",
    "                # Memory growth must be set before GPUs have been initialized\n",
    "                print(e)\n",
    "            \n",
    "        # setup logging\n",
    "        if self.config['logging_wandb']:\n",
    "            # w&b \n",
    "            wandb.init(name=name_run, \n",
    "                   project=self.project_name,\n",
    "                   notes=notes, \n",
    "                   tags=tags,\n",
    "                   entity='cv-task-2')\n",
    "\n",
    "            # save usefull config to w&b\n",
    "            wandb.config.learning_rate = self.config['train_parameters']['learning_rate']\n",
    "            wandb.config.batch_size = self.config['train_parameters']['batch_size']\n",
    "            wandb.config.epochs = self.config['train_parameters']['epochs']\n",
    "            wandb.config.steps_per_epoch = self.config['train_parameters']['steps_per_epoch']\n",
    "             \n",
    "        # build model \n",
    "        self.build()\n",
    "\n",
    "        # set model parts trainable or not\n",
    "        if self.config['train_base_model'] == False: \n",
    "            print('freezing base model layers')\n",
    "            for layer in self.base_model.layers:\n",
    "                layer.trainable = False\n",
    "        if self.config['train_head_model'] == False: \n",
    "            print('freezing head model layers')\n",
    "            for layer in self.head_model.layers:\n",
    "                layer.trainable = False\n",
    "        \n",
    "        \n",
    "        # compile model\n",
    "        self.compile_model()\n",
    "        \n",
    "        if self.config['logging_wandb']:\n",
    "            # set save_model true if you want wandb to upload weights once run has finished (takes some time)\n",
    "            clbcks = [WandbCallback(save_model=False)]\n",
    "        else: \n",
    "            clbcks = []\n",
    "\n",
    "        \n",
    "        # start training \n",
    "        history=self.model.fit(\n",
    "                    x = self.dataset.train_generator(batch_size=self.config['train_parameters']['batch_size']),\n",
    "                    steps_per_epoch = self.config['train_parameters']['steps_per_epoch'],\n",
    "                    epochs=self.config['train_parameters']['epochs'], \n",
    "                    validation_data=self.dataset.validation_generator(batch_size=self.config['train_parameters']['batch_size']),\n",
    "                    validation_steps=20, \n",
    "                    callbacks=clbcks\n",
    "        )\n",
    "        \n",
    "        #workers=multiprocessing.cpu_count(),\n",
    "        #use_multiprocessing=True,\n",
    "    \n",
    "    def prepare_for_inference(self, model_weights_path): \n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                # Currently, memory growth needs to be the same across GPUs\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "            except RuntimeError as e:\n",
    "                # Memory growth must be set before GPUs have been initialized\n",
    "                print(e)\n",
    "        self.build()\n",
    "        self.model.load_weights(model_weights_path)\n",
    "    \n",
    "    def show_heatmap_prediction(self, image_id):\n",
    "        LAYER_NAME=self.heatmap_layer_name\n",
    "        im = np.load('data/test/img/test_{}.npy'.format(image_id))\n",
    "        pre_im = self.dataset.prepare_test_image(im)\n",
    "        batch = np.expand_dims(pre_im, axis=0)\n",
    "    \n",
    "        pred = self.model.predict(batch)\n",
    "        idx=np.argmax(pred)\n",
    "        score = np.round(pred[0][idx]/np.sum(pred),4)\n",
    "        label=self.dataset.label_names[idx]\n",
    "\n",
    "        grad_model = tf.keras.models.Model([self.model.inputs], [self.model.get_layer(LAYER_NAME).output, self.model.output])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = grad_model(batch)\n",
    "            loss = predictions[:, idx]\n",
    "\n",
    "        output = conv_outputs[0]\n",
    "        grads = tape.gradient(loss, conv_outputs)[0]\n",
    "\n",
    "        gate_f = tf.cast(output > 0, 'float32')\n",
    "        gate_r = tf.cast(grads > 0, 'float32')\n",
    "        guided_grads = tf.cast(output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n",
    "\n",
    "        weights = tf.reduce_mean(guided_grads, axis=(0, 1))\n",
    "\n",
    "        cam = np.ones(output.shape[0: 2], dtype = np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * output[:, :, i]\n",
    "\n",
    "        cam = cv2.resize(cam.numpy(), (224, 224))\n",
    "        cam = np.maximum(cam, 0)\n",
    "        heatmap = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "\n",
    "        cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "        og_im = cv2.cvtColor(im.astype('uint8'), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        og_im = cv2.resize(og_im, (224, 224))\n",
    "\n",
    "\n",
    "        output_image = cv2.addWeighted(og_im, 0.7, cam, 1, 0)\n",
    "\n",
    "\n",
    "        fig, axes = plt.subplots(1,2, figsize=(30,15))\n",
    "        axes[1].imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].imshow(im)\n",
    "        axes[0].set_title('prediction: {}, score: {}'.format(label, np.round(100*score,2)), fontsize=25)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee1149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionModel(ClassifactionModel): \n",
    "    def __init__(self, config):\n",
    "        # setup model name for wandb\n",
    "        self.project_name = 'Xception'\n",
    "        self.heatmap_layer_name='block14_sepconv2_act'\n",
    "        # define the base model\n",
    "        self.base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                               include_top=False)\n",
    "        # super takes care of the rest\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # feed preprocessor function \n",
    "        self.dataset.feed_preprocess_function(keras.applications.xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37534ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'name': 'XceptionModel',\n",
    "    'logging_wandb': False,  #nice tool for tracking a run. make and account on wandb.ai and I will add you to this project\n",
    "    'weights': \"imagenet\", # 'imagenet', #None, \n",
    "    'nbr_classes': 20,\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_base_model': True, # whether to train the head and or base model\n",
    "    'train_head_model': True, \n",
    "    'train_parameters': {\n",
    "        'optimizer': 'ADAM',\n",
    "        'epochs': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.00001, \n",
    "        'steps_per_epoch': 2000\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    },\n",
    "    'head_model': {\n",
    "        'head_model_units': [], \n",
    "        'add_dropout':      [],\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35f1c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xception = XceptionModel(config)\n",
    "Xception.prepare_for_inference('weights/Xception_finetuned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8162c9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Xception.show_heatmap_prediction(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96087f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e577643",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Model(ClassifactionModel): \n",
    "    def __init__(self, config):\n",
    "        # setup model name for wandb\n",
    "        self.project_name = 'resnet50'\n",
    "        self.heatmap_layer_name='conv5_block3_out'\n",
    "        # define the base model\n",
    "        self.base_model = keras.applications.ResNet50V2(weights=\"imagenet\",\n",
    "                                                               include_top=False)\n",
    "        # super takes care of the rest\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # feed preprocessor function \n",
    "        self.dataset.feed_preprocess_function(keras.applications.resnet_v2.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_run='ResNet50_firstTry'\n",
    "notes='First try for finetuning resnet50 on pretrained imagenet weights. Data augmentation turned on. '\n",
    "tags = ['resnet50', 'head = []', 'head = []', 'Augmentation applied', 'uniform class distribution']\n",
    "resnet50=ResNet50Model(config)\n",
    "resnet50.train(name_run, notes, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd7b51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resnet50=ResNet50Model(config)\n",
    "resnet50.prepare_for_inference('weights/resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a3045d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resnet50.show_heatmap_prediction(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6367c9f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resnet50.model.get_layer('conv5_block3_out').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e753fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f64f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48f3614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6859c19f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66246ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0162e54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1275c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5f00b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN_venv",
   "language": "python",
   "name": "gan_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
