{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os \n",
    "import multiprocessing\n",
    "import wandb\n",
    "# !pip install wandb -qqq\n",
    "from wandb.keras import WandbCallback\n",
    "import kerastuner as kt #!python3.x -m pip install keras-tuner\n",
    "import cv2\n",
    "from ipywidgets import fixed, interact \n",
    "import ipywidgets\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
    "    RandomBrightness, RandomContrast, RandomGamma,\n",
    "    ToFloat, ShiftScaleRotate, RandomBrightnessContrast, RandomCrop)\n",
    "from data_utils import Dataset_Classification, Dataset_Segmentation\n",
    "import sys\n",
    "!pip install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
    "sys.path.append('./deeplab')\n",
    "from deeplabv3p import Deeplabv3\n",
    "from utils import SegModel, get_VOC2012_classes, Jaccard, sparse_accuracy_ignoring_last_label, sparse_crossentropy_ignoring_last_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-hunger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config = {\n",
    "    'train_fraction': 0.9,\n",
    "    'input_shape': (224, 224),\n",
    "    'augmentation': True, \n",
    "    'uniform_sample_probabilities': False\n",
    "}\n",
    "ds = Dataset_Classification(dataset_config)\n",
    "ds.show_class_distribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=0\n",
    "for (X,y) in ds.train_generator(10):\n",
    "    stop+=1\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    if stop>10: \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomClassificationModel:\n",
    "    \"\"\"\n",
    "    Random classification model: \n",
    "        - generates random labels for the inputs based on the class distribution observed during training\n",
    "        - assumes an input can have multiple labels\n",
    "    \"\"\"\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Adjusts the class ratio variable to the one observed in y. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "        y: list of arrays - n x (nb_classes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        self.distribution = np.mean(y, axis=0)\n",
    "        print(\"Setting class distribution to:\\n{}\".format(\"\\n\".join(f\"{label}: {p}\" for label, p in zip(labels, self.distribution))))\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts for each input a label.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X: list of arrays - n x (height x width x 3)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: list of arrays - n x (nb_classes)\n",
    "        \"\"\"\n",
    "        np.random.seed(0)\n",
    "        return [np.array([int(np.random.rand() < p) for p in self.distribution]) for _ in X]\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.predict(X)\n",
    "    \n",
    "    \n",
    "class ClassifactionModel(RandomClassificationModel): \n",
    "    \"\"\"\n",
    "        Main class implementing all functions necessary to train and/or use a classification model \n",
    "        This class has to be overwritten for each specific model of interest, where the base model should be implemented.\n",
    "    \"\"\"\n",
    "    def __init__(self, config): \n",
    "        self.config = config \n",
    "        self.config_head = config['head_model']\n",
    "        \n",
    "        # initialize dataset\n",
    "        self.dataset = Dataset_Classification(config['dataset'])\n",
    "              \n",
    "        # check if some configurations make sense \n",
    "        assert len(self.config_head['head_model_units']) == len(self.config_head['add_dropout']), 'head_models_units and add_dropout list should have same size'\n",
    "    \n",
    "    \n",
    "    def set_config(self, config):\n",
    "        self.config = config \n",
    "        self.config_head = config['head_model']\n",
    "        self.dataset = Dataset_Classification(config['dataset'])\n",
    "        assert len(self.config_head['head_model_units']) == len(self.config_head['add_dropout']), 'head_models_units and add_dropout list should have same size'\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # \n",
    "        \n",
    "        if len(X.shape) == 1: \n",
    "            # X is a batch of images prepare all of them and create batch. \n",
    "            batch = np.array([self.dataset.prepare_test_image(im) for im in X])\n",
    "            y = model.predict(batch)\n",
    "        else: \n",
    "            # X is a single image \n",
    "            batch = self.dataset.prepare_test_image(X)\n",
    "            batch = np.expand_dims(batch, axis=0)\n",
    "            y = model.predict(batch)\n",
    "            \n",
    "        y = np.squeeze(y)\n",
    "\n",
    "        label_idx = np.where(y==1.)\n",
    "\n",
    "        return self.dataset.label_names[label_idx]\n",
    "            \n",
    "    def build(self): \n",
    "        \"\"\"\n",
    "            Builds the model \n",
    "        \"\"\"\n",
    "#       self.base_model = resnet50\n",
    "        \n",
    "        # define a head model\n",
    "        head_model=keras.layers.GlobalAveragePooling2D()(self.base_model.output)\n",
    "\n",
    "        for (nbr_units, dropout) in zip(self.config_head['head_model_units'], self.config_head['add_dropout']): \n",
    "            head_model=tf.keras.layers.Dense(nbr_units, activation=self.config_head['activation'])(head_model)\n",
    "            if dropout:\n",
    "                head_model=tf.keras.layers.Dropout(0.4)(head_model)\n",
    "        \n",
    "        head_model=keras.layers.Dense(20, activation='softmax')(head_model)\n",
    "        #self.config['nbr_classes']\n",
    "        self.head_model = head_model\n",
    "                  \n",
    "        # combine both models \n",
    "        self.model = keras.Model(self.base_model.input, head_model)\n",
    "\n",
    "\n",
    "#         avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "#         output = keras.layers.Dense(20, activation='softmax')(avg)\n",
    "#         model=keras.Model(inputs=base_model.input, outputs=output)\n",
    "           \n",
    "    def compile_model(self): \n",
    "        # optimizer\n",
    "        if self.config['train_parameters']['optimizer'] == 'SGD':\n",
    "            optimizer = tf.keras.optimizers.SGD(\n",
    "                    learning_rate=self.config['train_parameters']['learning_rate'], momentum=0.9,\n",
    "                    nesterov=False, name=\"SGD\"\n",
    "                )\n",
    "        elif self.config['train_parameters']['optimizer'] == 'ADAM':\n",
    "            optimizer = tf.keras.optimizers.Adam(lr=self.config['train_parameters']['learning_rate'])\n",
    "\n",
    "        # metric\n",
    "        metrics = [tf.keras.metrics.CategoricalAccuracy(),\n",
    "                  tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top 3 categorical acccuracy'), \n",
    "                  tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top 5 categorical acccuracy')\n",
    "                  ]\n",
    "        \n",
    "        # loss\n",
    "        loss='categorical_crossentropy'\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "        \n",
    "        \n",
    "    def train(self, name_run, notes, tags):\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                # Currently, memory growth needs to be the same across GPUs\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "            except RuntimeError as e:\n",
    "                # Memory growth must be set before GPUs have been initialized\n",
    "                print(e)\n",
    "            \n",
    "        # setup logging\n",
    "        if self.config['logging_wandb']:\n",
    "            # w&b \n",
    "            wandb.init(name=name_run, \n",
    "                   project=self.project_name,\n",
    "                   notes=notes, \n",
    "                   tags=tags,\n",
    "                   entity='cv-task-2')\n",
    "\n",
    "            # save usefull config to w&b\n",
    "            wandb.config.learning_rate = self.config['train_parameters']['learning_rate']\n",
    "            wandb.config.batch_size = self.config['train_parameters']['batch_size']\n",
    "            wandb.config.epochs = self.config['train_parameters']['epochs']\n",
    "            wandb.config.steps_per_epoch = self.config['train_parameters']['steps_per_epoch']\n",
    "             \n",
    "        # build model \n",
    "        self.build()\n",
    "\n",
    "        # set model parts trainable or not\n",
    "        if self.config['train_base_model'] == False: \n",
    "            print('freezing base model layers')\n",
    "            for layer in self.base_model.layers:\n",
    "                layer.trainable = False\n",
    "        if self.config['train_head_model'] == False: \n",
    "            print('freezing head model layers')\n",
    "            for layer in self.head_model.layers:\n",
    "                layer.trainable = False\n",
    "        \n",
    "        \n",
    "        # compile model\n",
    "        self.compile_model()\n",
    "        \n",
    "        if self.config['logging_wandb']:\n",
    "            # set save_model true if you want wandb to upload weights once run has finished (takes some time)\n",
    "            clbcks = [WandbCallback(save_model=False)]\n",
    "        else: \n",
    "            clbcks = []\n",
    "\n",
    "        \n",
    "        # start training \n",
    "        history=self.model.fit(\n",
    "                    x = self.dataset.train_generator(batch_size=self.config['train_parameters']['batch_size']),\n",
    "                    steps_per_epoch = self.config['train_parameters']['steps_per_epoch'],\n",
    "                    epochs=self.config['train_parameters']['epochs'], \n",
    "                    validation_data=self.dataset.validation_generator(batch_size=self.config['train_parameters']['batch_size']),\n",
    "                    validation_steps=20, \n",
    "                    callbacks=clbcks\n",
    "        )\n",
    "        \n",
    "        #workers=multiprocessing.cpu_count(),\n",
    "        #use_multiprocessing=True,\n",
    "    \n",
    "    def prepare_for_inference(self, model_weights_path): \n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                # Currently, memory growth needs to be the same across GPUs\n",
    "                for gpu in gpus:\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "                logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "                print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "            except RuntimeError as e:\n",
    "                # Memory growth must be set before GPUs have been initialized\n",
    "                print(e)\n",
    "        self.build()\n",
    "        self.model.load_weights(model_weights_path)\n",
    "    \n",
    "    def show_heatmap_prediction(self, image_id):\n",
    "        LAYER_NAME=self.heatmap_layer_name\n",
    "        im = np.load('data/test/img/test_{}.npy'.format(image_id))\n",
    "        pre_im = self.dataset.prepare_test_image(im)\n",
    "        batch = np.expand_dims(pre_im, axis=0)\n",
    "    \n",
    "        pred = self.model.predict(batch)\n",
    "        idx=np.argmax(pred)\n",
    "        score = np.round(pred[0][idx]/np.sum(pred),4)\n",
    "        label=self.dataset.label_names[idx]\n",
    "\n",
    "        grad_model = tf.keras.models.Model([self.model.inputs], [self.model.get_layer(LAYER_NAME).output, self.model.output])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            conv_outputs, predictions = grad_model(batch)\n",
    "            loss = predictions[:, idx]\n",
    "\n",
    "        output = conv_outputs[0]\n",
    "        grads = tape.gradient(loss, conv_outputs)[0]\n",
    "\n",
    "        gate_f = tf.cast(output > 0, 'float32')\n",
    "        gate_r = tf.cast(grads > 0, 'float32')\n",
    "        guided_grads = tf.cast(output > 0, 'float32') * tf.cast(grads > 0, 'float32') * grads\n",
    "\n",
    "        weights = tf.reduce_mean(guided_grads, axis=(0, 1))\n",
    "\n",
    "        cam = np.ones(output.shape[0: 2], dtype = np.float32)\n",
    "\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * output[:, :, i]\n",
    "\n",
    "        cam = cv2.resize(cam.numpy(), (224, 224))\n",
    "        cam = np.maximum(cam, 0)\n",
    "        heatmap = (cam - cam.min()) / (cam.max() - cam.min())\n",
    "\n",
    "        cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "        og_im = cv2.cvtColor(im.astype('uint8'), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        og_im = cv2.resize(og_im, (224, 224))\n",
    "\n",
    "\n",
    "        output_image = cv2.addWeighted(og_im, 0.7, cam, 1, 0)\n",
    "\n",
    "\n",
    "        fig, axes = plt.subplots(1,2, figsize=(30,15))\n",
    "        axes[1].imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "        axes[0].imshow(im)\n",
    "        axes[0].set_title('prediction: {}, score: {}'.format(label, np.round(100*score,2)), fontsize=25)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XceptionModel(ClassifactionModel): \n",
    "    def __init__(self, config):\n",
    "        # setup model name for wandb\n",
    "        self.project_name = 'Xception'\n",
    "        self.heatmap_layer_name='block14_sepconv2_act'\n",
    "        # define the base model\n",
    "        self.base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                               include_top=False)\n",
    "        # super takes care of the rest\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # feed preprocessor function \n",
    "        self.dataset.feed_preprocess_function(keras.applications.xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'name': 'XceptionModel',\n",
    "    'logging_wandb': False,  #nice tool for tracking a run. make and account on wandb.ai and I will add you to this project\n",
    "    'weights': \"imagenet\", # 'imagenet', #None, \n",
    "    'nbr_classes': 20,\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_base_model': True, # whether to train the head and or base model\n",
    "    'train_head_model': True, \n",
    "    'train_parameters': {\n",
    "        'optimizer': 'ADAM',\n",
    "        'epochs': 5,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.00001, \n",
    "        'steps_per_epoch': 2000\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    },\n",
    "    'head_model': {\n",
    "        'head_model_units': [], \n",
    "        'add_dropout':      [],\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cellular-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xception = XceptionModel(config)\n",
    "Xception.prepare_for_inference('weights/Xception_finetuned.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-turkish",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xception.show_heatmap_prediction(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-royal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50Model(ClassifactionModel): \n",
    "    def __init__(self, config):\n",
    "        # setup model name for wandb\n",
    "        self.project_name = 'resnet50'\n",
    "        self.heatmap_layer_name='conv5_block3_out'\n",
    "        # define the base model\n",
    "        self.base_model = keras.applications.ResNet50V2(weights=\"imagenet\",\n",
    "                                                               include_top=False)\n",
    "        # super takes care of the rest\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # feed preprocessor function \n",
    "        self.dataset.feed_preprocess_function(keras.applications.resnet_v2.preprocess_input)\n",
    "config = {\n",
    "    'name': 'ResNet50',\n",
    "    'logging_wandb': False,  #nice tool for tracking a run. make and account on wandb.ai and I will add you to this project\n",
    "    'weights': \"imagenet\", # 'imagenet', #None, \n",
    "    'nbr_classes': 20,\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_base_model': False, # whether to train the head and or base model\n",
    "    'train_head_model': True, \n",
    "    'train_parameters': {\n",
    "        'optimizer': 'ADAM',\n",
    "        'epochs': 3,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.000001, \n",
    "        'steps_per_epoch': 2000\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    },\n",
    "    'head_model': {\n",
    "        'head_model_units': [1024, 1024], \n",
    "        'add_dropout':      [False, False],\n",
    "        'activation': 'relu'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_run='ResNet50_firstTry'\n",
    "notes='First try for finetuning resnet50 on pretrained imagenet weights. Data augmentation turned on. '\n",
    "tags = ['resnet50', 'head = []', 'head = []', 'Augmentation applied', 'uniform class distribution']\n",
    "resnet50=ResNet50Model(config)\n",
    "resnet50.train(name_run, notes, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecological-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50=ResNet50Model(config)\n",
    "resnet50.prepare_for_inference('weights/resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=109\n",
    "resnet50.show_heatmap_prediction(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-washer",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.model.get_layer('conv5_block3_out').output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet50.model.summary()\n",
    "resnet50.model.save_weights('resnet50_noOverfitting.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-services",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hairy-whole",
   "metadata": {},
   "source": [
    "# segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentationodel(ClassifactionModel): \n",
    "    \"\"\"\n",
    "        Main class implementing all functions necessary to train and/or use a classification model \n",
    "        This class has to be overwritten for each specific model of interest, where the base model should be implemented.\n",
    "    \"\"\"\n",
    "    def __init__(self, config): \n",
    "        self.config = config \n",
    "\n",
    "        # initialize dataset\n",
    "        self.dataset = Dataset_Segmentation(config['dataset'])\n",
    "              \n",
    "\n",
    "    def set_config(self, config):\n",
    "        self.config = config \n",
    "        self.dataset = Dataset_Segmentation(config['dataset'])\n",
    "    \n",
    "    def show_heatmap_prediction(self, image_id):\n",
    "        print('No')\n",
    "        \n",
    "    def predict(self, X, show=False):\n",
    "        og_size = X.shape\n",
    "        resized, _ = self.dataset.prepare_test_image(X, None)\n",
    "        batch = np.expand_dims(resized, axis=0)\n",
    "        y = self.model.predict(batch)\n",
    "        mask = np.argmax(y, -1)[0].reshape(224,224)\n",
    "        #stacked_img = np.array(np.stack((mask,)*3, axis=-1), dtype=np.float32)\n",
    "        #print(stacked_img.shape)\n",
    "        og_mask = cv2.resize(mask.astype('float32'), (og_size[1], og_size[0]))\n",
    "                             \n",
    "        class_names = get_VOC2012_classes()\n",
    "        class_ids = np.unique(mask.reshape(-1))\n",
    "                             \n",
    "        if show:\n",
    "            fig, axes = plt.subplots(figsize=(20,10))\n",
    "            axes.imshow(X)\n",
    "            axes.imshow(og_mask, alpha=0.5)\n",
    "            for idx in class_ids:\n",
    "                px_idx = np.where(og_mask == idx)\n",
    "                px=np.random.choice(px_idx[0])\n",
    "                py=np.random.choice(px_idx[1])\n",
    "                axes.text(px,py, '{}'.format(class_names[idx]), fontsize=15)\n",
    "            fig.show()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-bracelet",
   "metadata": {},
   "source": [
    "# deeplab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-broadcast",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deepLabModel(Segmentationodel):\n",
    "    def __init__(self, config): \n",
    "        self.project_name = 'deepLab'\n",
    "        super().__init__(config)\n",
    "    \n",
    "    def build(self):\n",
    "        model = Deeplabv3(weights=None, input_tensor=None, infer=False,\n",
    "                          input_shape=self.config['input_shape'], classes=21,\n",
    "                          backbone=self.config['backbone'], OS=16, alpha=1)\n",
    "        \n",
    "        base_model = keras.Model(model.input, model.layers[-5].output)\n",
    "        self.base_model = base_model\n",
    "        \n",
    "        if self.config['backbone']=='xception':\n",
    "            scale = 4\n",
    "        else:\n",
    "            scale = 8\n",
    "        sz=self.config['input_shape']\n",
    "        x = tf.keras.layers.Conv2D(21, (1, 1), padding='same', name='conv_upsample')(base_model.output)\n",
    "        x = tf.keras.layers.Lambda(lambda x: tf.compat.v1.image.resize_bilinear(x,size=(sz[0],sz[1])))(x)\n",
    "        x = tf.keras.layers.Reshape((sz[0]*sz[1], -1)) (x)\n",
    "        x = tf.keras.layers.Activation('softmax', name = 'pred_mask')(x)\n",
    "        model = keras.Model(base_model.input, x, name='deeplabv3p')\n",
    "\n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def compile(self):\n",
    "        # optimizer\n",
    "        if self.config['train_parameters']['optimizer'] == 'SGD':\n",
    "            optimizer = tf.keras.optimizers.SGD(\n",
    "                    learning_rate=self.config['train_parameters']['learning_rate'], momentum=0.9,\n",
    "                    nesterov=False, name=\"SGD\"\n",
    "                )\n",
    "        elif self.config['train_parameters']['optimizer'] == 'ADAM':\n",
    "            optimizer = tf.keras.optimizers.Adam(lr=self.config['train_parameters']['learning_rate'], \n",
    "                                                 epsilon=1e-8, decay=1e-6)\n",
    "\n",
    "        # metric\n",
    "        metrics = {'pred_mask' : [Jaccard, sparse_accuracy_ignoring_last_label]}\n",
    "        \n",
    "        # loss\n",
    "        loss = sparse_crossentropy_ignoring_last_label\n",
    "        \n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "config = {\n",
    "    'name': 'deepLab',\n",
    "    'logging_wandb': False,  #nice tool for tracking a run. make and account on wandb.ai and I will add you to this project\n",
    "    'nbr_classes': 20,\n",
    "    'input_shape': (224, 224, 3),\n",
    "    'train_base_model': True, # whether to train the head and or base model\n",
    "    'train_head_model': True,\n",
    "    'backbone': 'mobilenetv2', #xception\n",
    "    'train_parameters': {\n",
    "        'optimizer': 'ADAM',\n",
    "        'epochs': 3,\n",
    "        'batch_size': 64,\n",
    "        'learning_rate': 0.000001, \n",
    "        'steps_per_epoch': 2000\n",
    "    },\n",
    "    'dataset': {\n",
    "        'train_fraction': 0.9,\n",
    "        'input_shape': (224, 224),\n",
    "        'augmentation': True, # whether to augment images or not\n",
    "        'uniform_sample_probabilities': False\n",
    "    }\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-holder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "seg = deepLabModel(config)\n",
    "i=0\n",
    "for (X,y) in seg.dataset.train_generator(10): \n",
    "#     print(y.shape)\n",
    "#     new_y=list()\n",
    "#     for yy in y: \n",
    "#         new_y.append(to_categorical(yy, num_classes=21))\n",
    "#     y = np.array(new_y)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    i+=1\n",
    "    if i > 3: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_run='deepLab'\n",
    "notes='deeplab for segmentation'\n",
    "tags = ['deepLab', 'Augmentation applied', 'uniform class distribution']\n",
    "seg = deepLabModel(config)\n",
    "seg.train(name_run, notes, tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-springer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elegant-review",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg.prepare_for_inference('weights/mobilenetv2_original.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id=26\n",
    "im = np.load('data/test/img/test_{}.npy'.format(image_id))\n",
    "m=seg.predict(im, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-cream",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-approval",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-disney",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sufficient-wages",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-assembly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-sydney",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-worry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-guitar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('deeplab/')\n",
    "from deeplab.deeplabv3p import Deeplabv3\n",
    "from deeplab.utils import SegModel, get_VOC2012_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = SegModel('', image_size=(224,224))\n",
    "model = seg.create_seg_model(net='original',n=21, load_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-government",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id=6\n",
    "im = np.load('data/test/img/test_{}.npy'.format(image_id))\n",
    "plt.imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset_Segmentation(config['dataset'])\n",
    "resized, _ = ds.prepare_test_image(im, None)\n",
    "batch = np.expand_dims(resized, axis=0)\n",
    "\n",
    "y = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-stone",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.argmax(y, -1)[0].reshape(224,224)\n",
    "plt.imshow(mask)\n",
    "class_names = get_VOC2012_classes()\n",
    "class_ids = np.unique(mask.reshape(-1))\n",
    "for idx in class_ids:\n",
    "    px_idx = np.where(mask == idx)\n",
    "    px=np.random.choice(px_idx[0])\n",
    "    py=np.random.choice(px_idx[1])\n",
    "    plt.text(px,py, '{}'.format(class_names[idx]), fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-delay",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
